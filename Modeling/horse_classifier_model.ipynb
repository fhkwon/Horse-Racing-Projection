{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSFDbemsz5bJ",
    "outputId": "a65c494e-3600-4d04-d047-82e9ade12ab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (5.7.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from nbformat) (4.7.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from nbformat) (5.1.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from nbformat) (4.17.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.19.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from jsonschema>=2.6->nbformat) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from jsonschema>=2.6->nbformat) (5.10.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from jsonschema>=2.6->nbformat) (1.3.10)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat) (3.4.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\sjm\\appdata\\local\\programs\\orange\\lib\\site-packages (from jupyter-core->nbformat) (228)\n"
     ]
    }
   ],
   "source": [
    "# 설치 라이브러리 \n",
    "# ! pip install optuna\n",
    "# ! pip install xgboost\n",
    "# ! pip install lightgbm\n",
    "# ! pip install ipykernel\n",
    "! pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "70X5s-Oud_1e"
   },
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 시각화 라이브러리 \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 모델 평가 라이브러리\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "# 전처리 라이브러리\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 모델링 라이브러리\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost \n",
    "\n",
    "# 모델 파라미터 \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import optuna\n",
    "\n",
    "# 모델 저장\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# 기본 설정\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "7GRqOSsv6kUy"
   },
   "outputs": [],
   "source": [
    "# 그래프 한글 표현\n",
    "plt.rc('font', family='Malgun Gothic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "Q5zsS4gpycy4"
   },
   "outputs": [],
   "source": [
    "# 데이터 호출\n",
    "path = \"full_df(classifier).csv\"\n",
    "data_DF = pd.read_csv(path, encoding = \"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "YA9_5o-_E_bx"
   },
   "outputs": [],
   "source": [
    "# Unnamed: 0 컬럼 제거\n",
    "data_DF = data_DF.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>부마성적</th>\n",
       "      <th>모마성적</th>\n",
       "      <th>조교사 점수</th>\n",
       "      <th>기수 점수</th>\n",
       "      <th>거리</th>\n",
       "      <th>군(등급)</th>\n",
       "      <th>주로상태</th>\n",
       "      <th>부담중량</th>\n",
       "      <th>마체중</th>\n",
       "      <th>순위 점수</th>\n",
       "      <th>부담중량1</th>\n",
       "      <th>마체중1</th>\n",
       "      <th>순위 점수1</th>\n",
       "      <th>조교사 점수1</th>\n",
       "      <th>기수 점수1</th>\n",
       "      <th>부마성적1</th>\n",
       "      <th>모마성적1</th>\n",
       "      <th>부담중량2</th>\n",
       "      <th>마체중2</th>\n",
       "      <th>순위 점수2</th>\n",
       "      <th>조교사 점수2</th>\n",
       "      <th>기수 점수2</th>\n",
       "      <th>부마성적2</th>\n",
       "      <th>모마성적2</th>\n",
       "      <th>부담중량3</th>\n",
       "      <th>마체중3</th>\n",
       "      <th>순위 점수3</th>\n",
       "      <th>조교사 점수3</th>\n",
       "      <th>기수 점수3</th>\n",
       "      <th>부마성적3</th>\n",
       "      <th>모마성적3</th>\n",
       "      <th>부담중량4</th>\n",
       "      <th>마체중4</th>\n",
       "      <th>순위 점수4</th>\n",
       "      <th>조교사 점수4</th>\n",
       "      <th>기수 점수4</th>\n",
       "      <th>부마성적4</th>\n",
       "      <th>모마성적4</th>\n",
       "      <th>부담중량5</th>\n",
       "      <th>마체중5</th>\n",
       "      <th>순위 점수5</th>\n",
       "      <th>조교사 점수5</th>\n",
       "      <th>기수 점수5</th>\n",
       "      <th>부마성적5</th>\n",
       "      <th>모마성적5</th>\n",
       "      <th>부담중량6</th>\n",
       "      <th>마체중6</th>\n",
       "      <th>순위 점수6</th>\n",
       "      <th>조교사 점수6</th>\n",
       "      <th>기수 점수6</th>\n",
       "      <th>부마성적6</th>\n",
       "      <th>모마성적6</th>\n",
       "      <th>부담중량7</th>\n",
       "      <th>마체중7</th>\n",
       "      <th>순위 점수7</th>\n",
       "      <th>조교사 점수7</th>\n",
       "      <th>기수 점수7</th>\n",
       "      <th>부마성적7</th>\n",
       "      <th>모마성적7</th>\n",
       "      <th>부담중량8</th>\n",
       "      <th>마체중8</th>\n",
       "      <th>순위 점수8</th>\n",
       "      <th>조교사 점수8</th>\n",
       "      <th>기수 점수8</th>\n",
       "      <th>부마성적8</th>\n",
       "      <th>모마성적8</th>\n",
       "      <th>부담중량9</th>\n",
       "      <th>마체중9</th>\n",
       "      <th>순위 점수9</th>\n",
       "      <th>조교사 점수9</th>\n",
       "      <th>기수 점수9</th>\n",
       "      <th>부마성적9</th>\n",
       "      <th>모마성적9</th>\n",
       "      <th>부담중량10</th>\n",
       "      <th>마체중10</th>\n",
       "      <th>순위 점수10</th>\n",
       "      <th>조교사 점수10</th>\n",
       "      <th>기수 점수10</th>\n",
       "      <th>부마성적10</th>\n",
       "      <th>모마성적10</th>\n",
       "      <th>부담중량11</th>\n",
       "      <th>마체중11</th>\n",
       "      <th>순위 점수11</th>\n",
       "      <th>조교사 점수11</th>\n",
       "      <th>기수 점수11</th>\n",
       "      <th>부마성적11</th>\n",
       "      <th>모마성적11</th>\n",
       "      <th>부담중량12</th>\n",
       "      <th>마체중12</th>\n",
       "      <th>순위 점수12</th>\n",
       "      <th>조교사 점수12</th>\n",
       "      <th>기수 점수12</th>\n",
       "      <th>부마성적12</th>\n",
       "      <th>모마성적12</th>\n",
       "      <th>부담중량13</th>\n",
       "      <th>마체중13</th>\n",
       "      <th>순위 점수13</th>\n",
       "      <th>조교사 점수13</th>\n",
       "      <th>기수 점수13</th>\n",
       "      <th>부마성적13</th>\n",
       "      <th>모마성적13</th>\n",
       "      <th>target 순위</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>6.120297</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.056784</td>\n",
       "      <td>3</td>\n",
       "      <td>0.562395</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>24.312500</td>\n",
       "      <td>18.047619</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.244167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>18.222222</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.336826</td>\n",
       "      <td>6</td>\n",
       "      <td>0.817388</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.228511</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>23.071429</td>\n",
       "      <td>52.5</td>\n",
       "      <td>6.306275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>13.2</td>\n",
       "      <td>53.5</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.2186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.17368</td>\n",
       "      <td>16.357143</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.188264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.312500</td>\n",
       "      <td>18.047619</td>\n",
       "      <td>0.562395</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.056784</td>\n",
       "      <td>3</td>\n",
       "      <td>54.5</td>\n",
       "      <td>6.120297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.244167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>18.222222</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.336826</td>\n",
       "      <td>6</td>\n",
       "      <td>0.817388</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.228511</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>23.071429</td>\n",
       "      <td>52.5</td>\n",
       "      <td>6.306275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>13.2</td>\n",
       "      <td>53.5</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.2186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.17368</td>\n",
       "      <td>16.357143</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.188264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.184211</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>1</td>\n",
       "      <td>54.5</td>\n",
       "      <td>6.120297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.056784</td>\n",
       "      <td>3</td>\n",
       "      <td>0.562395</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>24.312500</td>\n",
       "      <td>18.047619</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.244167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>18.222222</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.336826</td>\n",
       "      <td>6</td>\n",
       "      <td>0.817388</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.228511</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>23.071429</td>\n",
       "      <td>52.5</td>\n",
       "      <td>6.306275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>13.2</td>\n",
       "      <td>53.5</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.2186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.17368</td>\n",
       "      <td>16.357143</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.188264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.222222</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.244167</td>\n",
       "      <td>0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>6.120297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.056784</td>\n",
       "      <td>3</td>\n",
       "      <td>0.562395</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>24.312500</td>\n",
       "      <td>18.047619</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.336826</td>\n",
       "      <td>6</td>\n",
       "      <td>0.817388</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.228511</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>23.071429</td>\n",
       "      <td>52.5</td>\n",
       "      <td>6.306275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>13.2</td>\n",
       "      <td>53.5</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.2186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.17368</td>\n",
       "      <td>16.357143</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.188264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>0.817388</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.336826</td>\n",
       "      <td>6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>6.120297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.056784</td>\n",
       "      <td>3</td>\n",
       "      <td>0.562395</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>24.312500</td>\n",
       "      <td>18.047619</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.244167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>18.222222</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.228511</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>23.071429</td>\n",
       "      <td>52.5</td>\n",
       "      <td>6.306275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>13.2</td>\n",
       "      <td>53.5</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.2186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.17368</td>\n",
       "      <td>16.357143</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.188264</td>\n",
       "      <td>6</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>52.863636</td>\n",
       "      <td>6.202555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343872</td>\n",
       "      <td>0.125041</td>\n",
       "      <td>19.064066</td>\n",
       "      <td>12.289275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        부마성적       모마성적    조교사 점수     기수 점수  거리  군(등급)  주로상태  부담중량       마체중  \\\n",
       "0  20.166667   0.333333  0.260867  0.375748  14      1     0  54.5  6.120297   \n",
       "1  24.312500  18.047619  0.562395  0.013551  14      1     0  51.0  6.056784   \n",
       "2  29.184211  10.226855  0.000000  0.135597  14      1     0  52.0  6.180017   \n",
       "3  18.222222  11.111111  0.254518  0.052286  14      1     0  52.0  6.244167   \n",
       "4  11.000000  10.226855  0.817388  0.185337  14      1     0  52.0  6.336826   \n",
       "\n",
       "   순위 점수  부담중량1      마체중1  순위 점수1   조교사 점수1    기수 점수1      부마성적1      모마성적1  \\\n",
       "0      0   51.0  6.056784       3  0.562395  0.013551  24.312500  18.047619   \n",
       "1      3   54.5  6.120297       0  0.260867  0.375748  20.166667   0.333333   \n",
       "2      1   54.5  6.120297       0  0.260867  0.375748  20.166667   0.333333   \n",
       "3      0   54.5  6.120297       0  0.260867  0.375748  20.166667   0.333333   \n",
       "4      6   54.5  6.120297       0  0.260867  0.375748  20.166667   0.333333   \n",
       "\n",
       "   부담중량2      마체중2  순위 점수2   조교사 점수2    기수 점수2      부마성적2      모마성적2  부담중량3  \\\n",
       "0   52.0  6.180017       1  0.000000  0.135597  29.184211  10.226855   52.0   \n",
       "1   52.0  6.180017       1  0.000000  0.135597  29.184211  10.226855   52.0   \n",
       "2   51.0  6.056784       3  0.562395  0.013551  24.312500  18.047619   52.0   \n",
       "3   51.0  6.056784       3  0.562395  0.013551  24.312500  18.047619   52.0   \n",
       "4   51.0  6.056784       3  0.562395  0.013551  24.312500  18.047619   52.0   \n",
       "\n",
       "       마체중3  순위 점수3   조교사 점수3    기수 점수3      부마성적3      모마성적3  부담중량4  \\\n",
       "0  6.244167       0  0.254518  0.052286  18.222222  11.111111   52.0   \n",
       "1  6.244167       0  0.254518  0.052286  18.222222  11.111111   52.0   \n",
       "2  6.244167       0  0.254518  0.052286  18.222222  11.111111   52.0   \n",
       "3  6.180017       1  0.000000  0.135597  29.184211  10.226855   52.0   \n",
       "4  6.180017       1  0.000000  0.135597  29.184211  10.226855   52.0   \n",
       "\n",
       "       마체중4  순위 점수4   조교사 점수4    기수 점수4      부마성적4      모마성적4  부담중량5  \\\n",
       "0  6.336826       6  0.817388  0.185337  11.000000  10.226855   52.0   \n",
       "1  6.336826       6  0.817388  0.185337  11.000000  10.226855   52.0   \n",
       "2  6.336826       6  0.817388  0.185337  11.000000  10.226855   52.0   \n",
       "3  6.336826       6  0.817388  0.185337  11.000000  10.226855   52.0   \n",
       "4  6.244167       0  0.254518  0.052286  18.222222  11.111111   52.0   \n",
       "\n",
       "       마체중5  순위 점수5   조교사 점수5    기수 점수5  부마성적5  모마성적5  부담중량6      마체중6  \\\n",
       "0  6.100319       0  0.289072  0.068179    6.0    9.4   52.0  6.228511   \n",
       "1  6.100319       0  0.289072  0.068179    6.0    9.4   52.0  6.228511   \n",
       "2  6.100319       0  0.289072  0.068179    6.0    9.4   52.0  6.228511   \n",
       "3  6.100319       0  0.289072  0.068179    6.0    9.4   52.0  6.228511   \n",
       "4  6.100319       0  0.289072  0.068179    6.0    9.4   52.0  6.228511   \n",
       "\n",
       "   순위 점수6   조교사 점수6    기수 점수6      부마성적6      모마성적6  부담중량7      마체중7  순위 점수7  \\\n",
       "0       0  0.143361  0.064983  29.184211  23.071429   52.5  6.306275       0   \n",
       "1       0  0.143361  0.064983  29.184211  23.071429   52.5  6.306275       0   \n",
       "2       0  0.143361  0.064983  29.184211  23.071429   52.5  6.306275       0   \n",
       "3       0  0.143361  0.064983  29.184211  23.071429   52.5  6.306275       0   \n",
       "4       0  0.143361  0.064983  29.184211  23.071429   52.5  6.306275       0   \n",
       "\n",
       "    조교사 점수7    기수 점수7      부마성적7  모마성적7  부담중량8      마체중8  순위 점수8   조교사 점수8  \\\n",
       "0  0.042539  0.065642  20.111111   13.2   53.5  6.248043       0  0.469562   \n",
       "1  0.042539  0.065642  20.111111   13.2   53.5  6.248043       0  0.469562   \n",
       "2  0.042539  0.065642  20.111111   13.2   53.5  6.248043       0  0.469562   \n",
       "3  0.042539  0.065642  20.111111   13.2   53.5  6.248043       0  0.469562   \n",
       "4  0.042539  0.065642  20.111111   13.2   53.5  6.248043       0  0.469562   \n",
       "\n",
       "     기수 점수8  부마성적8      모마성적8  부담중량9    마체중9  순위 점수9   조교사 점수9   기수 점수9  \\\n",
       "0  0.044859   15.0  10.226855   58.0  6.2186       0  0.625851  0.17368   \n",
       "1  0.044859   15.0  10.226855   58.0  6.2186       0  0.625851  0.17368   \n",
       "2  0.044859   15.0  10.226855   58.0  6.2186       0  0.625851  0.17368   \n",
       "3  0.044859   15.0  10.226855   58.0  6.2186       0  0.625851  0.17368   \n",
       "4  0.044859   15.0  10.226855   58.0  6.2186       0  0.625851  0.17368   \n",
       "\n",
       "       부마성적9      모마성적9  부담중량10     마체중10  순위 점수10  조교사 점수10   기수 점수10  \\\n",
       "0  16.357143  19.111111    52.0  6.188264        6  0.317036  0.195587   \n",
       "1  16.357143  19.111111    52.0  6.188264        6  0.317036  0.195587   \n",
       "2  16.357143  19.111111    52.0  6.188264        6  0.317036  0.195587   \n",
       "3  16.357143  19.111111    52.0  6.188264        6  0.317036  0.195587   \n",
       "4  16.357143  19.111111    52.0  6.188264        6  0.317036  0.195587   \n",
       "\n",
       "      부마성적10     모마성적10     부담중량11     마체중11  순위 점수11  조교사 점수11   기수 점수11  \\\n",
       "0  20.166667  10.226855  52.863636  6.202555        0  0.343872  0.125041   \n",
       "1  20.166667  10.226855  52.863636  6.202555        0  0.343872  0.125041   \n",
       "2  20.166667  10.226855  52.863636  6.202555        0  0.343872  0.125041   \n",
       "3  20.166667  10.226855  52.863636  6.202555        0  0.343872  0.125041   \n",
       "4  20.166667  10.226855  52.863636  6.202555        0  0.343872  0.125041   \n",
       "\n",
       "      부마성적11     모마성적11     부담중량12     마체중12  순위 점수12  조교사 점수12   기수 점수12  \\\n",
       "0  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "1  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "2  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "3  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "4  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "\n",
       "      부마성적12     모마성적12     부담중량13     마체중13  순위 점수13  조교사 점수13   기수 점수13  \\\n",
       "0  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "1  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "2  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "3  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "4  19.064066  12.289275  52.863636  6.202555        0  0.343872  0.125041   \n",
       "\n",
       "      부마성적13     모마성적13  target 순위  \n",
       "0  19.064066  12.289275          1  \n",
       "1  19.064066  12.289275          1  \n",
       "2  19.064066  12.289275          1  \n",
       "3  19.064066  12.289275          0  \n",
       "4  19.064066  12.289275          0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZV-r6qRXg4A"
   },
   "source": [
    "# Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TafY7KZIXYuK"
   },
   "source": [
    "## train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6991\n",
       "1    3202\n",
       "Name: target 순위, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_DF[\"target 순위\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "ylJMb4K7kM63"
   },
   "outputs": [],
   "source": [
    "# 종속변수 / 독리변수 분해\n",
    "x_horse = data_DF.drop(columns =[\"target 순위\"])\n",
    "y_horse = data_DF[\"target 순위\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "v8iQqwHDeQ2f"
   },
   "outputs": [],
   "source": [
    "# 학습 데이터, 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_horse, y_horse, test_size=0.35, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>부마성적</th>\n",
       "      <th>모마성적</th>\n",
       "      <th>조교사 점수</th>\n",
       "      <th>기수 점수</th>\n",
       "      <th>거리</th>\n",
       "      <th>군(등급)</th>\n",
       "      <th>주로상태</th>\n",
       "      <th>부담중량</th>\n",
       "      <th>마체중</th>\n",
       "      <th>순위 점수</th>\n",
       "      <th>부담중량1</th>\n",
       "      <th>마체중1</th>\n",
       "      <th>순위 점수1</th>\n",
       "      <th>조교사 점수1</th>\n",
       "      <th>기수 점수1</th>\n",
       "      <th>부마성적1</th>\n",
       "      <th>모마성적1</th>\n",
       "      <th>부담중량2</th>\n",
       "      <th>마체중2</th>\n",
       "      <th>순위 점수2</th>\n",
       "      <th>조교사 점수2</th>\n",
       "      <th>기수 점수2</th>\n",
       "      <th>부마성적2</th>\n",
       "      <th>모마성적2</th>\n",
       "      <th>부담중량3</th>\n",
       "      <th>마체중3</th>\n",
       "      <th>순위 점수3</th>\n",
       "      <th>조교사 점수3</th>\n",
       "      <th>기수 점수3</th>\n",
       "      <th>부마성적3</th>\n",
       "      <th>모마성적3</th>\n",
       "      <th>부담중량4</th>\n",
       "      <th>마체중4</th>\n",
       "      <th>순위 점수4</th>\n",
       "      <th>조교사 점수4</th>\n",
       "      <th>기수 점수4</th>\n",
       "      <th>부마성적4</th>\n",
       "      <th>모마성적4</th>\n",
       "      <th>부담중량5</th>\n",
       "      <th>마체중5</th>\n",
       "      <th>순위 점수5</th>\n",
       "      <th>조교사 점수5</th>\n",
       "      <th>기수 점수5</th>\n",
       "      <th>부마성적5</th>\n",
       "      <th>모마성적5</th>\n",
       "      <th>부담중량6</th>\n",
       "      <th>마체중6</th>\n",
       "      <th>순위 점수6</th>\n",
       "      <th>조교사 점수6</th>\n",
       "      <th>기수 점수6</th>\n",
       "      <th>부마성적6</th>\n",
       "      <th>모마성적6</th>\n",
       "      <th>부담중량7</th>\n",
       "      <th>마체중7</th>\n",
       "      <th>순위 점수7</th>\n",
       "      <th>조교사 점수7</th>\n",
       "      <th>기수 점수7</th>\n",
       "      <th>부마성적7</th>\n",
       "      <th>모마성적7</th>\n",
       "      <th>부담중량8</th>\n",
       "      <th>마체중8</th>\n",
       "      <th>순위 점수8</th>\n",
       "      <th>조교사 점수8</th>\n",
       "      <th>기수 점수8</th>\n",
       "      <th>부마성적8</th>\n",
       "      <th>모마성적8</th>\n",
       "      <th>부담중량9</th>\n",
       "      <th>마체중9</th>\n",
       "      <th>순위 점수9</th>\n",
       "      <th>조교사 점수9</th>\n",
       "      <th>기수 점수9</th>\n",
       "      <th>부마성적9</th>\n",
       "      <th>모마성적9</th>\n",
       "      <th>부담중량10</th>\n",
       "      <th>마체중10</th>\n",
       "      <th>순위 점수10</th>\n",
       "      <th>조교사 점수10</th>\n",
       "      <th>기수 점수10</th>\n",
       "      <th>부마성적10</th>\n",
       "      <th>모마성적10</th>\n",
       "      <th>부담중량11</th>\n",
       "      <th>마체중11</th>\n",
       "      <th>순위 점수11</th>\n",
       "      <th>조교사 점수11</th>\n",
       "      <th>기수 점수11</th>\n",
       "      <th>부마성적11</th>\n",
       "      <th>모마성적11</th>\n",
       "      <th>부담중량12</th>\n",
       "      <th>마체중12</th>\n",
       "      <th>순위 점수12</th>\n",
       "      <th>조교사 점수12</th>\n",
       "      <th>기수 점수12</th>\n",
       "      <th>부마성적12</th>\n",
       "      <th>모마성적12</th>\n",
       "      <th>부담중량13</th>\n",
       "      <th>마체중13</th>\n",
       "      <th>순위 점수13</th>\n",
       "      <th>조교사 점수13</th>\n",
       "      <th>기수 점수13</th>\n",
       "      <th>부마성적13</th>\n",
       "      <th>모마성적13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4823</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.030303</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>0.279554</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.142037</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.107023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817388</td>\n",
       "      <td>0.366437</td>\n",
       "      <td>22.090909</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.182085</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042539</td>\n",
       "      <td>0.094697</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>14.166667</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.118097</td>\n",
       "      <td>7</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.135222</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>19.090909</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.154858</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492124</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.043447</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.018593</td>\n",
       "      <td>1</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.132307</td>\n",
       "      <td>24.066667</td>\n",
       "      <td>15.125000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.182085</td>\n",
       "      <td>4</td>\n",
       "      <td>0.567563</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.120297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>0.756750</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.111111</td>\n",
       "      <td>54.888889</td>\n",
       "      <td>6.132043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387924</td>\n",
       "      <td>0.224173</td>\n",
       "      <td>18.955485</td>\n",
       "      <td>11.232090</td>\n",
       "      <td>54.888889</td>\n",
       "      <td>6.132043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387924</td>\n",
       "      <td>0.224173</td>\n",
       "      <td>18.955485</td>\n",
       "      <td>11.232090</td>\n",
       "      <td>54.888889</td>\n",
       "      <td>6.132043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387924</td>\n",
       "      <td>0.224173</td>\n",
       "      <td>18.955485</td>\n",
       "      <td>11.232090</td>\n",
       "      <td>54.888889</td>\n",
       "      <td>6.132043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387924</td>\n",
       "      <td>0.224173</td>\n",
       "      <td>18.955485</td>\n",
       "      <td>11.232090</td>\n",
       "      <td>54.888889</td>\n",
       "      <td>6.132043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387924</td>\n",
       "      <td>0.224173</td>\n",
       "      <td>18.955485</td>\n",
       "      <td>11.232090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>0.414096</td>\n",
       "      <td>0.279554</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.214608</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.107023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375088</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.230481</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.038560</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.023448</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254518</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>22.028571</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.159095</td>\n",
       "      <td>0</td>\n",
       "      <td>0.393885</td>\n",
       "      <td>0.135222</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>29.176471</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.133398</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410571</td>\n",
       "      <td>0.153274</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.133333</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.154858</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.032447</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.139885</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339363</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>13.086957</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.145350</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.227439</td>\n",
       "      <td>21.687201</td>\n",
       "      <td>13.541594</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.145350</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.227439</td>\n",
       "      <td>21.687201</td>\n",
       "      <td>13.541594</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.145350</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.227439</td>\n",
       "      <td>21.687201</td>\n",
       "      <td>13.541594</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.145350</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.227439</td>\n",
       "      <td>21.687201</td>\n",
       "      <td>13.541594</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.145350</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.227439</td>\n",
       "      <td>21.687201</td>\n",
       "      <td>13.541594</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.145350</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.227439</td>\n",
       "      <td>21.687201</td>\n",
       "      <td>13.541594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>33.173913</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.084730</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.126869</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.111467</td>\n",
       "      <td>6</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>0.399724</td>\n",
       "      <td>26.062500</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.156979</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>28.083333</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.236370</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>53.5</td>\n",
       "      <td>6.146329</td>\n",
       "      <td>3</td>\n",
       "      <td>0.443473</td>\n",
       "      <td>0.153274</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.093750</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>6.164434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313411</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>23.721674</td>\n",
       "      <td>13.786895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.562395</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.156979</td>\n",
       "      <td>10</td>\n",
       "      <td>0.103175</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>20.181818</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.154858</td>\n",
       "      <td>2</td>\n",
       "      <td>0.264621</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.181818</td>\n",
       "      <td>54.5</td>\n",
       "      <td>6.216606</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040653</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>24.066667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.218600</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030554</td>\n",
       "      <td>0.037524</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.194405</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339363</td>\n",
       "      <td>0.211667</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>6.104793</td>\n",
       "      <td>0</td>\n",
       "      <td>0.432895</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>12.190476</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>6.175867</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.028781</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>6.226537</td>\n",
       "      <td>0</td>\n",
       "      <td>0.205402</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>3</td>\n",
       "      <td>0.375088</td>\n",
       "      <td>0.097703</td>\n",
       "      <td>22.090909</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>53.150000</td>\n",
       "      <td>6.180868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.062086</td>\n",
       "      <td>19.268427</td>\n",
       "      <td>7.562230</td>\n",
       "      <td>53.150000</td>\n",
       "      <td>6.180868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.062086</td>\n",
       "      <td>19.268427</td>\n",
       "      <td>7.562230</td>\n",
       "      <td>53.150000</td>\n",
       "      <td>6.180868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.062086</td>\n",
       "      <td>19.268427</td>\n",
       "      <td>7.562230</td>\n",
       "      <td>53.150000</td>\n",
       "      <td>6.180868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.062086</td>\n",
       "      <td>19.268427</td>\n",
       "      <td>7.562230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>23.275000</td>\n",
       "      <td>0.030554</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.167516</td>\n",
       "      <td>0</td>\n",
       "      <td>0.567563</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.190315</td>\n",
       "      <td>10</td>\n",
       "      <td>0.086481</td>\n",
       "      <td>0.369635</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.182085</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375088</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>22.090909</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.142037</td>\n",
       "      <td>5</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>58.151515</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.204558</td>\n",
       "      <td>5</td>\n",
       "      <td>0.398586</td>\n",
       "      <td>0.756750</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>16.179487</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.115892</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410571</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>12.037037</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.129050</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.084730</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>10.142857</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.159095</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.135222</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>13.125000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>6.075346</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040653</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>13.269231</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.144186</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>18.107143</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.240276</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.173680</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>54.750000</td>\n",
       "      <td>6.159473</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312395</td>\n",
       "      <td>0.285895</td>\n",
       "      <td>28.958949</td>\n",
       "      <td>12.311485</td>\n",
       "      <td>54.750000</td>\n",
       "      <td>6.159473</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312395</td>\n",
       "      <td>0.285895</td>\n",
       "      <td>28.958949</td>\n",
       "      <td>12.311485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9372</th>\n",
       "      <td>17.142857</td>\n",
       "      <td>17.055556</td>\n",
       "      <td>0.575789</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.173786</td>\n",
       "      <td>7</td>\n",
       "      <td>55.5</td>\n",
       "      <td>6.139885</td>\n",
       "      <td>4</td>\n",
       "      <td>0.562395</td>\n",
       "      <td>0.399724</td>\n",
       "      <td>24.312500</td>\n",
       "      <td>11.080000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.104793</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.030379</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>13.142857</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.272877</td>\n",
       "      <td>9</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.173680</td>\n",
       "      <td>24.222222</td>\n",
       "      <td>10.142857</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.118097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.756750</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>6.148148</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.107023</td>\n",
       "      <td>9</td>\n",
       "      <td>0.491655</td>\n",
       "      <td>0.166252</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>6.169611</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410571</td>\n",
       "      <td>0.055765</td>\n",
       "      <td>30.076923</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>54.428571</td>\n",
       "      <td>6.155153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>22.977916</td>\n",
       "      <td>11.081345</td>\n",
       "      <td>54.428571</td>\n",
       "      <td>6.155153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>22.977916</td>\n",
       "      <td>11.081345</td>\n",
       "      <td>54.428571</td>\n",
       "      <td>6.155153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>22.977916</td>\n",
       "      <td>11.081345</td>\n",
       "      <td>54.428571</td>\n",
       "      <td>6.155153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>22.977916</td>\n",
       "      <td>11.081345</td>\n",
       "      <td>54.428571</td>\n",
       "      <td>6.155153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>22.977916</td>\n",
       "      <td>11.081345</td>\n",
       "      <td>54.428571</td>\n",
       "      <td>6.155153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>22.977916</td>\n",
       "      <td>11.081345</td>\n",
       "      <td>54.428571</td>\n",
       "      <td>6.155153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>22.977916</td>\n",
       "      <td>11.081345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7291</th>\n",
       "      <td>23.090909</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.169611</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.210600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410571</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>12.352941</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.095825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046762</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.144186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.117647</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.202536</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264621</td>\n",
       "      <td>0.083410</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.129050</td>\n",
       "      <td>0</td>\n",
       "      <td>0.432895</td>\n",
       "      <td>0.084730</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.089045</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.038560</td>\n",
       "      <td>15.090909</td>\n",
       "      <td>16.179487</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.082219</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.042320</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.052632</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>22.266667</td>\n",
       "      <td>13.090909</td>\n",
       "      <td>0.491655</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.253829</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.118097</td>\n",
       "      <td>3</td>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>38.068966</td>\n",
       "      <td>11.157895</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.224558</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339363</td>\n",
       "      <td>0.369635</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>55.5</td>\n",
       "      <td>6.180017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040653</td>\n",
       "      <td>0.173680</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.206576</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143361</td>\n",
       "      <td>0.135222</td>\n",
       "      <td>11.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>54.5</td>\n",
       "      <td>6.142037</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>0</td>\n",
       "      <td>0.228434</td>\n",
       "      <td>0.064983</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>6.175867</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260867</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>6.193628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.233764</td>\n",
       "      <td>17.125883</td>\n",
       "      <td>9.045684</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>6.193628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.233764</td>\n",
       "      <td>17.125883</td>\n",
       "      <td>9.045684</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>6.193628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.233764</td>\n",
       "      <td>17.125883</td>\n",
       "      <td>9.045684</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>6.193628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.233764</td>\n",
       "      <td>17.125883</td>\n",
       "      <td>9.045684</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>6.193628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.233764</td>\n",
       "      <td>17.125883</td>\n",
       "      <td>9.045684</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>6.193628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.233764</td>\n",
       "      <td>17.125883</td>\n",
       "      <td>9.045684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.117647</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.144186</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.210600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410571</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>12.352941</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.169611</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.095825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046762</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.202536</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264621</td>\n",
       "      <td>0.083410</td>\n",
       "      <td>23.090909</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.129050</td>\n",
       "      <td>0</td>\n",
       "      <td>0.432895</td>\n",
       "      <td>0.084730</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>31.105263</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.089045</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.038560</td>\n",
       "      <td>15.090909</td>\n",
       "      <td>16.179487</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.082219</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.042320</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.052632</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "      <td>54.777778</td>\n",
       "      <td>6.135932</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343743</td>\n",
       "      <td>0.202336</td>\n",
       "      <td>22.085650</td>\n",
       "      <td>12.536956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>34.052632</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>3</td>\n",
       "      <td>55.5</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.065642</td>\n",
       "      <td>19.062500</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>55.5</td>\n",
       "      <td>6.190315</td>\n",
       "      <td>10</td>\n",
       "      <td>0.375088</td>\n",
       "      <td>0.756750</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.248043</td>\n",
       "      <td>8</td>\n",
       "      <td>0.289072</td>\n",
       "      <td>0.094697</td>\n",
       "      <td>17.090909</td>\n",
       "      <td>24.147059</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.259581</td>\n",
       "      <td>2</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>18.153846</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.293419</td>\n",
       "      <td>13</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>0.369635</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>6.212606</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.185337</td>\n",
       "      <td>16.357143</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>6.230481</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469562</td>\n",
       "      <td>0.166252</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>6.190315</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.132307</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>51.195122</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>6.196444</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334853</td>\n",
       "      <td>0.135597</td>\n",
       "      <td>29.184211</td>\n",
       "      <td>10.226855</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>6.223839</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460948</td>\n",
       "      <td>0.328196</td>\n",
       "      <td>20.126235</td>\n",
       "      <td>17.088757</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>6.223839</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460948</td>\n",
       "      <td>0.328196</td>\n",
       "      <td>20.126235</td>\n",
       "      <td>17.088757</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>6.223839</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460948</td>\n",
       "      <td>0.328196</td>\n",
       "      <td>20.126235</td>\n",
       "      <td>17.088757</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>6.223839</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460948</td>\n",
       "      <td>0.328196</td>\n",
       "      <td>20.126235</td>\n",
       "      <td>17.088757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6625 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           부마성적       모마성적    조교사 점수     기수 점수  거리  군(등급)  주로상태  부담중량  \\\n",
       "4823   6.000000  11.030303  0.334853  0.279554  12      5     0  55.0   \n",
       "3716  14.000000  10.226855  0.414096  0.279554  12      6     3  54.0   \n",
       "1176  16.000000  33.173913  0.289072  0.084730  14      5     2  54.0   \n",
       "4609  18.000000   3.333333  0.562395  0.015802  14      5     0  51.0   \n",
       "234   16.000000  23.275000  0.030554  0.135597  14      4     1  58.0   \n",
       "...         ...        ...       ...       ...  ..    ...   ...   ...   \n",
       "9372  17.142857  17.055556  0.575789  0.185337  13      5     2  55.0   \n",
       "7291  23.090909  10.226855  0.469562  0.044859  10      6     0  56.0   \n",
       "1344  22.266667  13.090909  0.491655  0.044859  12      4     0  53.0   \n",
       "7293  11.000000  17.117647  0.334853  1.000000  10      6     0  56.0   \n",
       "1289  34.052632  10.226855  0.334853  1.000000  12      7     3  57.0   \n",
       "\n",
       "           마체중  순위 점수  부담중량1      마체중1  순위 점수1   조교사 점수1    기수 점수1      부마성적1  \\\n",
       "4823  6.142037      0   54.0  6.107023       0  0.817388  0.366437  22.090909   \n",
       "3716  6.214608      0   56.0  6.107023       0  0.375088  0.135597  23.090909   \n",
       "1176  6.126869      0   54.0  6.111467       6  0.334853  0.399724  26.062500   \n",
       "4609  6.180017      0   52.0  6.156979      10  0.103175  0.013551  20.181818   \n",
       "234   6.163315      9   54.0  6.167516       0  0.567563  0.195587  31.105263   \n",
       "...        ...    ...    ...       ...     ...       ...       ...        ...   \n",
       "9372  6.173786      7   55.5  6.139885       4  0.562395  0.399724  24.312500   \n",
       "7291  6.169611      0   54.0  6.210600       0  0.410571  0.135597  14.000000   \n",
       "1344  6.253829      0   56.0  6.118097       3  0.528317  0.013551  38.068966   \n",
       "7293  6.144186      0   54.0  6.210600       0  0.410571  0.135597  14.000000   \n",
       "1289  6.208590      3   55.5  6.208590       0  0.528317  0.065642  19.062500   \n",
       "\n",
       "          모마성적1  부담중량2      마체중2  순위 점수2   조교사 점수2    기수 점수2      부마성적2  \\\n",
       "4823  11.000000   55.0  6.182085       0  0.042539  0.094697  19.000000   \n",
       "3716  10.226855   54.0  6.230481       0  0.528317  0.038560  14.000000   \n",
       "1176  10.226855   57.0  6.208590       0  0.143361  0.195587  29.184211   \n",
       "4609  10.226855   52.0  6.154858       2  0.264621  0.064983  11.000000   \n",
       "234    6.200000   54.0  6.190315      10  0.086481  0.369635  17.142857   \n",
       "...         ...    ...       ...     ...       ...       ...        ...   \n",
       "9372  11.080000   52.0  6.104793       0  0.528317  0.030379  23.000000   \n",
       "7291  12.352941   51.0  6.095825       0  0.046762  0.015802  50.250000   \n",
       "1344  11.157895   52.0  6.224558       0  0.339363  0.369635   5.428571   \n",
       "7293  12.352941   56.0  6.169611       0  0.469562  0.044859  23.090909   \n",
       "1289  10.226855   55.5  6.190315      10  0.375088  0.756750   9.000000   \n",
       "\n",
       "          모마성적2  부담중량3      마체중3  순위 점수3   조교사 점수3    기수 점수3      부마성적3  \\\n",
       "4823  14.166667   56.0  6.118097       7  0.143361  0.135222  20.166667   \n",
       "3716  10.226855   56.0  6.023448       0  0.254518  1.000000  31.105263   \n",
       "1176   7.000000   56.0  6.156979       0  0.334853  0.044859  28.083333   \n",
       "4609  15.181818   54.5  6.216606       0  0.040653  0.052286  24.066667   \n",
       "234   10.226855   54.0  6.182085       0  0.375088  0.013551  22.090909   \n",
       "...         ...    ...       ...     ...       ...       ...        ...   \n",
       "9372  13.142857   57.0  6.272877       9  0.625851  0.173680  24.222222   \n",
       "7291  10.226855   56.0  6.144186       0  0.334853  1.000000  11.000000   \n",
       "1344  22.100000   55.5  6.180017       0  0.040653  0.173680   9.000000   \n",
       "7293  10.226855   51.0  6.095825       0  0.046762  0.015802  50.250000   \n",
       "1289  12.100000   57.0  6.248043       8  0.289072  0.094697  17.090909   \n",
       "\n",
       "          모마성적3  부담중량4      마체중4  순위 점수4   조교사 점수4    기수 점수4      부마성적4  \\\n",
       "4823  19.090909   50.0  6.154858       0  0.492124  0.013551  23.090909   \n",
       "3716  22.028571   54.0  6.159095       0  0.393885  0.135222  23.090909   \n",
       "1176   7.000000   53.0  6.236370       0  0.334853  0.015802  23.000000   \n",
       "4609   3.000000   54.0  6.218600       3  0.030554  0.037524  17.142857   \n",
       "234    0.111111   55.0  6.142037       5  0.260867  0.375748  58.151515   \n",
       "...         ...    ...       ...     ...       ...       ...        ...   \n",
       "9372  10.142857   53.0  6.118097       5  0.289072  0.756750  23.090909   \n",
       "7291  17.117647   54.0  6.202536       0  0.264621  0.083410  23.090909   \n",
       "1344   6.000000   52.0  6.206576       0  0.143361  0.135222  11.142857   \n",
       "7293  10.226855   54.0  6.202536       0  0.264621  0.083410  23.090909   \n",
       "1289  24.147059   57.0  6.259581       2  0.317036  0.375748  18.153846   \n",
       "\n",
       "          모마성적4  부담중량5      마체중5  순위 점수5   조교사 점수5    기수 점수5      부마성적5  \\\n",
       "4823  10.226855   56.0  6.163315       0  0.469562  0.043447  29.184211   \n",
       "3716  29.176471   56.0  6.133398       0  0.410571  0.153274   6.000000   \n",
       "1176  10.226855   53.5  6.146329       3  0.443473  0.153274  20.000000   \n",
       "4609  10.226855   52.0  6.194405       0  0.339363  0.211667  14.000000   \n",
       "234   19.166667   55.0  6.204558       5  0.398586  0.756750  23.090909   \n",
       "...         ...    ...       ...     ...       ...       ...        ...   \n",
       "9372   6.148148   55.0  6.107023       9  0.491655  0.166252  19.000000   \n",
       "7291  17.142857   56.0  6.129050       0  0.432895  0.084730  17.142857   \n",
       "1344   0.333333   54.5  6.142037       3  0.625851  1.000000  16.100000   \n",
       "7293  17.142857   56.0  6.129050       0  0.432895  0.084730  17.142857   \n",
       "1289  10.226855   57.0  6.293419      13  0.334853  0.369635  20.111111   \n",
       "\n",
       "          모마성적5      부담중량6      마체중6  순위 점수6   조교사 점수6    기수 점수6      부마성적6  \\\n",
       "4823  10.226855  56.000000  6.018593       1  0.289072  0.132307  24.066667   \n",
       "3716   3.133333  54.000000  6.154858       0  0.260867  0.032447  31.105263   \n",
       "1176  15.093750  54.583333  6.164434       0  0.313411  0.148996  23.721674   \n",
       "4609  10.226855  54.500000  6.104793       0  0.432895  0.032917  23.090909   \n",
       "234   16.179487  55.000000  6.115892       1  0.410571  0.185337  31.105263   \n",
       "...         ...        ...       ...     ...       ...       ...        ...   \n",
       "9372   7.000000  53.500000  6.169611       0  0.410571  0.055765  30.076923   \n",
       "7291   6.200000  56.000000  6.100319       0  0.317036  0.375748  31.105263   \n",
       "1344   0.500000  53.000000  6.248043       0  0.228434  0.064983  16.000000   \n",
       "7293   6.200000  56.000000  6.100319       0  0.317036  0.375748  31.105263   \n",
       "1289  13.200000  57.000000  6.212606       0  0.625851  0.185337  16.357143   \n",
       "\n",
       "          모마성적6      부담중량7      마체중7  순위 점수7   조교사 점수7    기수 점수7      부마성적7  \\\n",
       "4823  15.125000  56.000000  6.182085       4  0.567563  0.195587  11.000000   \n",
       "3716  10.226855  56.000000  6.139885       0  0.339363  0.044859  31.105263   \n",
       "1176  13.786895  54.583333  6.164434       0  0.313411  0.148996  23.721674   \n",
       "4609  12.190476  52.000000  6.175867       0  0.625851  0.028781  23.000000   \n",
       "234   12.037037  55.000000  6.129050       0  0.317036  0.084730  31.105263   \n",
       "...         ...        ...       ...     ...       ...       ...        ...   \n",
       "9372  13.000000  54.428571  6.155153       0  0.497664  0.252555  22.977916   \n",
       "7291   9.333333  56.000000  6.089045       0  0.528317  0.038560  15.090909   \n",
       "1344  16.100000  52.000000  6.175867       0  0.260867  0.068179  19.000000   \n",
       "7293   9.333333  56.000000  6.089045       0  0.528317  0.038560  15.090909   \n",
       "1289  19.111111  57.000000  6.230481       0  0.469562  0.166252  15.000000   \n",
       "\n",
       "          모마성적7      부담중량8      마체중8  순위 점수8   조교사 점수8    기수 점수8      부마성적8  \\\n",
       "4823   6.111111  56.000000  6.120297       0  0.334853  0.756750  16.000000   \n",
       "3716  13.086957  55.000000  6.145350       0  0.372088  0.227439  21.687201   \n",
       "1176  13.786895  54.583333  6.164434       0  0.313411  0.148996  23.721674   \n",
       "4609   8.125000  52.500000  6.226537       0  0.205402  0.065642  20.111111   \n",
       "234   10.142857  56.000000  6.159095       0  0.143361  0.135222  31.105263   \n",
       "...         ...        ...       ...     ...       ...       ...        ...   \n",
       "9372  11.081345  54.428571  6.155153       0  0.497664  0.252555  22.977916   \n",
       "7291  16.179487  54.000000  6.082219       0  0.289072  0.042320  14.000000   \n",
       "1344   3.083333  53.500000  6.193628       0  0.332313  0.233764  17.125883   \n",
       "7293  16.179487  54.000000  6.082219       0  0.289072  0.042320  14.000000   \n",
       "1289  10.226855  57.000000  6.190315       3  1.000000  0.132307  23.250000   \n",
       "\n",
       "          모마성적8      부담중량9      마체중9  순위 점수9   조교사 점수9    기수 점수9      부마성적9  \\\n",
       "4823   4.111111  54.888889  6.132043       0  0.387924  0.224173  18.955485   \n",
       "3716  13.541594  55.000000  6.145350       0  0.372088  0.227439  21.687201   \n",
       "1176  13.786895  54.583333  6.164434       0  0.313411  0.148996  23.721674   \n",
       "4609   3.000000  57.000000  6.180017       3  0.375088  0.097703  22.090909   \n",
       "234   13.125000  51.000000  6.075346       3  0.040653  0.004902  13.269231   \n",
       "...         ...        ...       ...     ...       ...       ...        ...   \n",
       "9372  11.081345  54.428571  6.155153       0  0.497664  0.252555  22.977916   \n",
       "7291  14.052632  54.777778  6.135932       0  0.343743  0.202336  22.085650   \n",
       "1344   9.045684  53.500000  6.193628       0  0.332313  0.233764  17.125883   \n",
       "7293  14.052632  54.777778  6.135932       0  0.343743  0.202336  22.085650   \n",
       "1289  51.195122  57.000000  6.196444       0  0.334853  0.135597  29.184211   \n",
       "\n",
       "          모마성적9     부담중량10     마체중10  순위 점수10  조교사 점수10   기수 점수10     부마성적10  \\\n",
       "4823  11.232090  54.888889  6.132043        0  0.387924  0.224173  18.955485   \n",
       "3716  13.541594  55.000000  6.145350        0  0.372088  0.227439  21.687201   \n",
       "1176  13.786895  54.583333  6.164434        0  0.313411  0.148996  23.721674   \n",
       "4609   0.111111  53.150000  6.180868        0  0.298000  0.062086  19.268427   \n",
       "234    0.166667  55.000000  6.144186        1  0.492122  1.000000  50.250000   \n",
       "...         ...        ...       ...      ...       ...       ...        ...   \n",
       "9372  11.081345  54.428571  6.155153        0  0.497664  0.252555  22.977916   \n",
       "7291  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1344   9.045684  53.500000  6.193628        0  0.332313  0.233764  17.125883   \n",
       "7293  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1289  10.226855  56.700000  6.223839        0  0.460948  0.328196  20.126235   \n",
       "\n",
       "         모마성적10     부담중량11     마체중11  순위 점수11  조교사 점수11   기수 점수11     부마성적11  \\\n",
       "4823  11.232090  54.888889  6.132043        0  0.387924  0.224173  18.955485   \n",
       "3716  13.541594  55.000000  6.145350        0  0.372088  0.227439  21.687201   \n",
       "1176  13.786895  54.583333  6.164434        0  0.313411  0.148996  23.721674   \n",
       "4609   7.562230  53.150000  6.180868        0  0.298000  0.062086  19.268427   \n",
       "234   18.107143  55.000000  6.240276        0  0.625851  0.173680  23.090909   \n",
       "...         ...        ...       ...      ...       ...       ...        ...   \n",
       "9372  11.081345  54.428571  6.155153        0  0.497664  0.252555  22.977916   \n",
       "7291  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1344   9.045684  53.500000  6.193628        0  0.332313  0.233764  17.125883   \n",
       "7293  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1289  17.088757  56.700000  6.223839        0  0.460948  0.328196  20.126235   \n",
       "\n",
       "         모마성적11     부담중량12     마체중12  순위 점수12  조교사 점수12   기수 점수12     부마성적12  \\\n",
       "4823  11.232090  54.888889  6.132043        0  0.387924  0.224173  18.955485   \n",
       "3716  13.541594  55.000000  6.145350        0  0.372088  0.227439  21.687201   \n",
       "1176  13.786895  54.583333  6.164434        0  0.313411  0.148996  23.721674   \n",
       "4609   7.562230  53.150000  6.180868        0  0.298000  0.062086  19.268427   \n",
       "234   19.000000  54.750000  6.159473        0  0.312395  0.285895  28.958949   \n",
       "...         ...        ...       ...      ...       ...       ...        ...   \n",
       "9372  11.081345  54.428571  6.155153        0  0.497664  0.252555  22.977916   \n",
       "7291  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1344   9.045684  53.500000  6.193628        0  0.332313  0.233764  17.125883   \n",
       "7293  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1289  17.088757  56.700000  6.223839        0  0.460948  0.328196  20.126235   \n",
       "\n",
       "         모마성적12     부담중량13     마체중13  순위 점수13  조교사 점수13   기수 점수13     부마성적13  \\\n",
       "4823  11.232090  54.888889  6.132043        0  0.387924  0.224173  18.955485   \n",
       "3716  13.541594  55.000000  6.145350        0  0.372088  0.227439  21.687201   \n",
       "1176  13.786895  54.583333  6.164434        0  0.313411  0.148996  23.721674   \n",
       "4609   7.562230  53.150000  6.180868        0  0.298000  0.062086  19.268427   \n",
       "234   12.311485  54.750000  6.159473        0  0.312395  0.285895  28.958949   \n",
       "...         ...        ...       ...      ...       ...       ...        ...   \n",
       "9372  11.081345  54.428571  6.155153        0  0.497664  0.252555  22.977916   \n",
       "7291  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1344   9.045684  53.500000  6.193628        0  0.332313  0.233764  17.125883   \n",
       "7293  12.536956  54.777778  6.135932        0  0.343743  0.202336  22.085650   \n",
       "1289  17.088757  56.700000  6.223839        0  0.460948  0.328196  20.126235   \n",
       "\n",
       "         모마성적13  \n",
       "4823  11.232090  \n",
       "3716  13.541594  \n",
       "1176  13.786895  \n",
       "4609   7.562230  \n",
       "234   12.311485  \n",
       "...         ...  \n",
       "9372  11.081345  \n",
       "7291  12.536956  \n",
       "1344   9.045684  \n",
       "7293  12.536956  \n",
       "1289  17.088757  \n",
       "\n",
       "[6625 rows x 101 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4539\n",
       "1    2086\n",
       "Name: target 순위, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2452\n",
       "1    1116\n",
       "Name: target 순위, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOl0R9mwQFDA",
    "outputId": "88caed43-3369-445e-b9bb-efcb2e9b108d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 6625\n",
      "test data 3568\n",
      "train label 6625\n",
      "test label 3568\n"
     ]
    }
   ],
   "source": [
    "# train / test 확인 \n",
    "print(\"train data\", len(X_train))\n",
    "print(\"test data\", len(X_test))\n",
    "print(\"train label\", len(y_train))\n",
    "print(\"test label\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold\n",
    "kfold = KFold(n_splits = 5, random_state = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "VDv-Ku1Sk6HR"
   },
   "outputs": [],
   "source": [
    "# StratifiedKFold\n",
    "S_kfold = StratifiedKFold(n_splits = 5, random_state = 10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "OGXdtlkAU7OD"
   },
   "outputs": [],
   "source": [
    "# RepeatedKFold\n",
    "R_kfold = RepeatedKFold(n_splits = 5, random_state = 10, n_repeats = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Hw3h9QWF-8"
   },
   "source": [
    "## Cross Validation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_socre(model):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"precision\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpsnwjAU35FM"
   },
   "source": [
    "#### RandomForestClassifier TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvQeV2B-t_MB",
    "outputId": "e7d20481-4201-4fcb-8490-ad566735d8b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF모델 0번째 검증 precision : 0.7635\n",
      "RF모델 1번째 검증 precision : 0.8148\n",
      "RF모델 2번째 검증 precision : 0.7431\n",
      "RF모델 3번째 검증 precision : 0.7847\n",
      "RF모델 4번째 검증 precision : 0.7950\n",
      "RF 평균 precision :  0.7802274324013455\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier \n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state = 29)\n",
    "sk_rf = cross_socre(rf)\n",
    "for idx, score in enumerate(sk_rf):\n",
    "    print(\"RF모델 {0}번째 검증 precision : {1:.4f}\".format(idx,score))\n",
    "print(\"RF 평균 precision : \", sk_rf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l_4Z4IkN3sJd",
    "outputId": "9731608a-f99e-4ad9-f715-9955e6a9ff9f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기수 점수 : 0.025344992945551658\n",
      "순위 점수 : 0.02468752690632303\n",
      "마체중3 : 0.021572785361166406\n",
      "모마성적3 : 0.019903870780539432\n",
      "기수 점수3 : 0.01863365260293735\n",
      "조교사 점수3 : 0.01789347438597896\n",
      "부마성적3 : 0.01776766146355733\n",
      "마체중 : 0.01753350671240787\n",
      "마체중2 : 0.017319375137044988\n",
      "부마성적2 : 0.01639461529511482\n"
     ]
    }
   ],
   "source": [
    "rf_best_feature = []\n",
    "rf.fit(X_train, y_train)\n",
    "sort = rf.feature_importances_\n",
    "for k, v in zip(X_train.columns, sort):\n",
    "    rf_best_feature.append([k,v])\n",
    "rf_best_sort = sorted(rf_best_feature, key=lambda x : x[1], reverse=True)\n",
    "for i,v in  rf_best_sort[:10]:\n",
    "    print(f'{i} : {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBClassifier TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning:\n",
      "\n",
      "`use_label_encoder` is deprecated in 1.7.0.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB모델 0번째 검증 precision:0.7257\n",
      "XGB모델 1번째 검증 precision:0.7576\n",
      "XGB모델 2번째 검증 precision:0.7216\n",
      "XGB모델 3번째 검증 precision:0.7703\n",
      "XGB모델 4번째 검증 precision:0.7163\n",
      "XGB 평균 precision: 0.738300136707249\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier \n",
    "xgbc = XGBClassifier(n_jobs = -1, random_state=1234, use_label_encoder=False)\n",
    "scores = cross_socre(xgbc)\n",
    "for idx, score in enumerate(scores):\n",
    "    print(\"XGB모델 {0}번째 검증 precision:{1:.4f}\".format(idx,score))\n",
    "print(\"XGB 평균 precision:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순위 점수 : 0.03776804357767105\n",
      "순위 점수9 : 0.034250058233737946\n",
      "순위 점수8 : 0.022608991712331772\n",
      "기수 점수13 : 0.0166267529129982\n",
      "순위 점수7 : 0.01617281883955002\n",
      "기수 점수12 : 0.015895210206508636\n",
      "부담중량13 : 0.015770496800541878\n",
      "기수 점수 : 0.01468771230429411\n",
      "거리 : 0.014655761420726776\n",
      "순위 점수6 : 0.013618232682347298\n"
     ]
    }
   ],
   "source": [
    "xgbc_best_feature = []\n",
    "xgbc.fit(X_train, y_train)\n",
    "sort = xgbc.feature_importances_\n",
    "for k, v in zip(X_train.columns, sort):\n",
    "    xgbc_best_feature.append([k,v])\n",
    "xgbc_best_sort = sorted(xgbc_best_feature, key=lambda x : x[1], reverse=True)\n",
    "for i,v in  xgbc_best_sort[:10]:\n",
    "    print(f'{i} : {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbYZdFIc4BOa"
   },
   "source": [
    "#### LGBMClassifier TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjOKRWodsVYL",
    "outputId": "9f9dc78a-2fb4-48d5-fc9b-dd8b0cef7e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB모델 0번째 검증 precision:0.6910\n",
      "LGB모델 1번째 검증 precision:0.7327\n",
      "LGB모델 2번째 검증 precision:0.7277\n",
      "LGB모델 3번째 검증 precision:0.7327\n",
      "LGB모델 4번째 검증 precision:0.7380\n",
      "LGB 평균 precision: 0.7244140165382893\n"
     ]
    }
   ],
   "source": [
    "# LGBMClassifier\n",
    "gbm = LGBMClassifier(n_jobs=-1, random_state = 10)\n",
    "sk_gbm = cross_socre(gbm)\n",
    "for idx, score in enumerate(sk_gbm):\n",
    "    print(\"LGB모델 {0}번째 검증 precision:{1:.4f}\".format(idx,score))\n",
    "print(\"LGB 평균 precision:\", sk_gbm.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MX4mw__k3ssA",
    "outputId": "6056ae06-b073-40fd-997c-393bcfaf8021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기수 점수3 : 0.0109\n",
      "모마성적3 : 0.0091\n",
      "마체중2 : 0.0083\n",
      "마체중3 : 0.0082\n",
      "조교사 점수3 : 0.0079\n",
      "부마성적3 : 0.0077\n",
      "마체중4 : 0.0074\n",
      "부마성적4 : 0.0066\n",
      "기수 점수 : 0.0065\n",
      "모마성적2 : 0.0063\n"
     ]
    }
   ],
   "source": [
    "gbm_best_feature = []\n",
    "gbm.fit(X_train, y_train)\n",
    "sort = gbm.feature_importances_ / 10000\n",
    "for k, v in zip(X_train.columns, sort):\n",
    "    gbm_best_feature.append([k,v])\n",
    "gbm_best_sort = sorted(gbm_best_feature, key=lambda x : x[1], reverse=True)\n",
    "for i,v in  gbm_best_sort[:10]:\n",
    "    print(f'{i} : {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D9dNxwRQohf"
   },
   "source": [
    "## Parameter check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBdJd0P_YGE7"
   },
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective (trial):\n",
    "    params = {\n",
    "      \"n_estimators\" : trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "      \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 10),\n",
    "      \"min_samples_split\" : trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "      \"max_leaf_nodes\" : trial.suggest_int(\"max_leaf_nodes\", 2, 100),\n",
    "      \"max_features\" : trial.suggest_int(\"max_features\", 1, X_train.shape[1])\n",
    "             }\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state = 100, **params)\n",
    "    socres = cross_socre(rf)\n",
    "    rf_score = socres.mean()\n",
    "    return rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-18 03:13:50,015]\u001b[0m A new study created in memory with name: no-name-82c564f3-547b-4434-a227-c6e0a572ce14\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:13:56,930]\u001b[0m Trial 0 finished with value: 0.5729265619291195 and parameters: {'n_estimators': 237, 'max_depth': 5, 'min_samples_split': 4, 'max_leaf_nodes': 10, 'max_features': 53}. Best is trial 0 with value: 0.5729265619291195.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:14:18,126]\u001b[0m Trial 1 finished with value: 0.6072442912511202 and parameters: {'n_estimators': 344, 'max_depth': 7, 'min_samples_split': 3, 'max_leaf_nodes': 52, 'max_features': 82}. Best is trial 1 with value: 0.6072442912511202.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:14:26,099]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'n_estimators': 956, 'max_depth': 5, 'min_samples_split': 9, 'max_leaf_nodes': 21, 'max_features': 6}. Best is trial 1 with value: 0.6072442912511202.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:14:30,139]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'n_estimators': 414, 'max_depth': 1, 'min_samples_split': 5, 'max_leaf_nodes': 26, 'max_features': 36}. Best is trial 1 with value: 0.6072442912511202.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:14:46,408]\u001b[0m Trial 4 finished with value: 0.5333333333333333 and parameters: {'n_estimators': 696, 'max_depth': 3, 'min_samples_split': 9, 'max_leaf_nodes': 98, 'max_features': 64}. Best is trial 1 with value: 0.6072442912511202.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:15:01,213]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'n_estimators': 739, 'max_depth': 3, 'min_samples_split': 5, 'max_leaf_nodes': 87, 'max_features': 53}. Best is trial 1 with value: 0.6072442912511202.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:15:56,984]\u001b[0m Trial 6 finished with value: 0.6135755314761473 and parameters: {'n_estimators': 969, 'max_depth': 7, 'min_samples_split': 7, 'max_leaf_nodes': 74, 'max_features': 78}. Best is trial 6 with value: 0.6135755314761473.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:16:22,599]\u001b[0m Trial 7 finished with value: 0.5932439375422122 and parameters: {'n_estimators': 599, 'max_depth': 9, 'min_samples_split': 2, 'max_leaf_nodes': 29, 'max_features': 58}. Best is trial 6 with value: 0.6135755314761473.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:16:39,859]\u001b[0m Trial 8 finished with value: 0.6008282450498439 and parameters: {'n_estimators': 189, 'max_depth': 9, 'min_samples_split': 2, 'max_leaf_nodes': 63, 'max_features': 96}. Best is trial 6 with value: 0.6135755314761473.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:16:44,555]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'n_estimators': 522, 'max_depth': 2, 'min_samples_split': 10, 'max_leaf_nodes': 29, 'max_features': 11}. Best is trial 6 with value: 0.6135755314761473.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:17:43,913]\u001b[0m Trial 10 finished with value: 0.6076946374271059 and parameters: {'n_estimators': 974, 'max_depth': 7, 'min_samples_split': 7, 'max_leaf_nodes': 76, 'max_features': 80}. Best is trial 6 with value: 0.6135755314761473.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:18:45,595]\u001b[0m Trial 11 finished with value: 0.6139108229524142 and parameters: {'n_estimators': 1000, 'max_depth': 7, 'min_samples_split': 7, 'max_leaf_nodes': 74, 'max_features': 81}. Best is trial 11 with value: 0.6139108229524142.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:19:46,492]\u001b[0m Trial 12 finished with value: 0.5957299166057843 and parameters: {'n_estimators': 830, 'max_depth': 7, 'min_samples_split': 7, 'max_leaf_nodes': 70, 'max_features': 98}. Best is trial 11 with value: 0.6139108229524142.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:20:38,814]\u001b[0m Trial 13 finished with value: 0.6098822288822289 and parameters: {'n_estimators': 868, 'max_depth': 10, 'min_samples_split': 7, 'max_leaf_nodes': 50, 'max_features': 75}. Best is trial 11 with value: 0.6139108229524142.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:21:19,029]\u001b[0m Trial 14 finished with value: 0.6041569557703278 and parameters: {'n_estimators': 845, 'max_depth': 6, 'min_samples_split': 8, 'max_leaf_nodes': 83, 'max_features': 71}. Best is trial 11 with value: 0.6139108229524142.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:21:49,334]\u001b[0m Trial 15 finished with value: 0.635405586929108 and parameters: {'n_estimators': 988, 'max_depth': 8, 'min_samples_split': 6, 'max_leaf_nodes': 55, 'max_features': 35}. Best is trial 15 with value: 0.635405586929108.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:22:12,633]\u001b[0m Trial 16 finished with value: 0.6226196581196581 and parameters: {'n_estimators': 735, 'max_depth': 9, 'min_samples_split': 5, 'max_leaf_nodes': 46, 'max_features': 36}. Best is trial 15 with value: 0.635405586929108.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:22:33,287]\u001b[0m Trial 17 finished with value: 0.6207417759374281 and parameters: {'n_estimators': 709, 'max_depth': 9, 'min_samples_split': 5, 'max_leaf_nodes': 44, 'max_features': 35}. Best is trial 15 with value: 0.635405586929108.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:22:51,354]\u001b[0m Trial 18 finished with value: 0.6186245150084743 and parameters: {'n_estimators': 580, 'max_depth': 10, 'min_samples_split': 4, 'max_leaf_nodes': 40, 'max_features': 36}. Best is trial 15 with value: 0.635405586929108.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:23:10,250]\u001b[0m Trial 19 finished with value: 0.6498424663434805 and parameters: {'n_estimators': 795, 'max_depth': 8, 'min_samples_split': 6, 'max_leaf_nodes': 60, 'max_features': 24}. Best is trial 19 with value: 0.6498424663434805.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:23:30,268]\u001b[0m Trial 20 finished with value: 0.6608806963645673 and parameters: {'n_estimators': 892, 'max_depth': 8, 'min_samples_split': 6, 'max_leaf_nodes': 60, 'max_features': 24}. Best is trial 20 with value: 0.6608806963645673.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:23:46,104]\u001b[0m Trial 21 finished with value: 0.6571162505465303 and parameters: {'n_estimators': 879, 'max_depth': 8, 'min_samples_split': 6, 'max_leaf_nodes': 58, 'max_features': 19}. Best is trial 20 with value: 0.6608806963645673.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:24:01,031]\u001b[0m Trial 22 finished with value: 0.6731011983185896 and parameters: {'n_estimators': 845, 'max_depth': 8, 'min_samples_split': 6, 'max_leaf_nodes': 61, 'max_features': 18}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:24:14,045]\u001b[0m Trial 23 finished with value: 0.5266666666666667 and parameters: {'n_estimators': 858, 'max_depth': 6, 'min_samples_split': 6, 'max_leaf_nodes': 63, 'max_features': 18}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:24:34,214]\u001b[0m Trial 24 finished with value: 0.6695668930963048 and parameters: {'n_estimators': 917, 'max_depth': 8, 'min_samples_split': 8, 'max_leaf_nodes': 67, 'max_features': 24}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:24:43,685]\u001b[0m Trial 25 finished with value: 0.0 and parameters: {'n_estimators': 640, 'max_depth': 4, 'min_samples_split': 8, 'max_leaf_nodes': 38, 'max_features': 26}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:25:12,793]\u001b[0m Trial 26 finished with value: 0.6356318112944619 and parameters: {'n_estimators': 784, 'max_depth': 8, 'min_samples_split': 8, 'max_leaf_nodes': 67, 'max_features': 45}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:25:17,376]\u001b[0m Trial 27 finished with value: 0.0 and parameters: {'n_estimators': 474, 'max_depth': 6, 'min_samples_split': 10, 'max_leaf_nodes': 86, 'max_features': 6}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:25:24,186]\u001b[0m Trial 28 finished with value: 0.2 and parameters: {'n_estimators': 909, 'max_depth': 10, 'min_samples_split': 9, 'max_leaf_nodes': 100, 'max_features': 2}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:25:26,161]\u001b[0m Trial 29 finished with value: 0.0 and parameters: {'n_estimators': 105, 'max_depth': 5, 'min_samples_split': 4, 'max_leaf_nodes': 80, 'max_features': 14}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:25:48,881]\u001b[0m Trial 30 finished with value: 0.6657614508253895 and parameters: {'n_estimators': 918, 'max_depth': 8, 'min_samples_split': 8, 'max_leaf_nodes': 93, 'max_features': 27}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:26:13,360]\u001b[0m Trial 31 finished with value: 0.6574370664796196 and parameters: {'n_estimators': 917, 'max_depth': 8, 'min_samples_split': 8, 'max_leaf_nodes': 87, 'max_features': 29}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:26:49,042]\u001b[0m Trial 32 finished with value: 0.635223925160634 and parameters: {'n_estimators': 797, 'max_depth': 9, 'min_samples_split': 8, 'max_leaf_nodes': 94, 'max_features': 45}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:26:57,630]\u001b[0m Trial 33 finished with value: 0.0 and parameters: {'n_estimators': 913, 'max_depth': 7, 'min_samples_split': 9, 'max_leaf_nodes': 2, 'max_features': 18}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:27:36,757]\u001b[0m Trial 34 finished with value: 0.6363499245852187 and parameters: {'n_estimators': 924, 'max_depth': 8, 'min_samples_split': 7, 'max_leaf_nodes': 67, 'max_features': 44}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:27:54,725]\u001b[0m Trial 35 finished with value: 0.6321840958605665 and parameters: {'n_estimators': 667, 'max_depth': 6, 'min_samples_split': 6, 'max_leaf_nodes': 93, 'max_features': 29}. Best is trial 22 with value: 0.6731011983185896.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:01,333]\u001b[0m Trial 36 finished with value: 0.6833333333333333 and parameters: {'n_estimators': 389, 'max_depth': 9, 'min_samples_split': 5, 'max_leaf_nodes': 52, 'max_features': 10}. Best is trial 36 with value: 0.6833333333333333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:07,672]\u001b[0m Trial 37 finished with value: 0.778888888888889 and parameters: {'n_estimators': 355, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 52, 'max_features': 12}. Best is trial 37 with value: 0.778888888888889.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:13,319]\u001b[0m Trial 38 finished with value: 0.8033333333333333 and parameters: {'n_estimators': 322, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 51, 'max_features': 11}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:28:18,575]\u001b[0m Trial 39 finished with value: 0.4 and parameters: {'n_estimators': 330, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 35, 'max_features': 10}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:28:22,423]\u001b[0m Trial 40 finished with value: 0.0 and parameters: {'n_estimators': 324, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 18, 'max_features': 7}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:29,992]\u001b[0m Trial 41 finished with value: 0.7155555555555555 and parameters: {'n_estimators': 399, 'max_depth': 9, 'min_samples_split': 3, 'max_leaf_nodes': 51, 'max_features': 13}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:28:33,537]\u001b[0m Trial 42 finished with value: 0.0 and parameters: {'n_estimators': 394, 'max_depth': 9, 'min_samples_split': 2, 'max_leaf_nodes': 51, 'max_features': 1}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:39,744]\u001b[0m Trial 43 finished with value: 0.7061157796451913 and parameters: {'n_estimators': 264, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 48, 'max_features': 16}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:44,343]\u001b[0m Trial 44 finished with value: 0.7814285714285714 and parameters: {'n_estimators': 277, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 45, 'max_features': 11}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:49,244]\u001b[0m Trial 45 finished with value: 0.7376748251748252 and parameters: {'n_estimators': 257, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 46, 'max_features': 14}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:28:53,544]\u001b[0m Trial 46 finished with value: 0.5619047619047619 and parameters: {'n_estimators': 255, 'max_depth': 10, 'min_samples_split': 4, 'max_leaf_nodes': 33, 'max_features': 13}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:29:10,535]\u001b[0m Trial 47 finished with value: 0.5960625326460744 and parameters: {'n_estimators': 218, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 43, 'max_features': 92}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:29:14,363]\u001b[0m Trial 48 finished with value: 0.0 and parameters: {'n_estimators': 445, 'max_depth': 9, 'min_samples_split': 2, 'max_leaf_nodes': 22, 'max_features': 2}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:29:16,902]\u001b[0m Trial 49 finished with value: 0.0 and parameters: {'n_estimators': 293, 'max_depth': 1, 'min_samples_split': 4, 'max_leaf_nodes': 54, 'max_features': 8}. Best is trial 38 with value: 0.8033333333333333.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# optuna study\n",
    "rf_study = optuna.create_study(direction = \"maximize\")\n",
    "rf_study.optimize(rf_objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : [0.8033333333333333]\n",
      "params : {'n_estimators': 322, 'max_depth': 10, 'min_samples_split': 3, 'max_leaf_nodes': 51, 'max_features': 11}\n"
     ]
    }
   ],
   "source": [
    "# 최적의 values 및 params\n",
    "print(\"value :\", rf_study.best_trial.values)\n",
    "print(\"params :\", rf_study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier : 0.00985663082437276\n"
     ]
    }
   ],
   "source": [
    "rf_study_best = RandomForestClassifier (\n",
    "                                      n_estimators = rf_study.best_trial.params[\"n_estimators\"], \n",
    "                                      max_depth = rf_study.best_trial.params[\"max_depth\"],\n",
    "                                      min_samples_split = rf_study.best_trial.params[\"min_samples_split\"], \n",
    "                                      max_leaf_nodes = rf_study.best_trial.params[\"max_leaf_nodes\"],\n",
    "                                      max_features = rf_study.best_trial.params[\"max_features\"],\n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=10\n",
    "                                        )\n",
    "\n",
    "rf_study_best.fit(X_train, y_train)\n",
    "rf_study_ypred = rf_study_best.predict(X_test)\n",
    "print(\"RandomForestClassifier :\", precision_score(rf_study_ypred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_model_best.pkl']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_study_best, \"rf_model_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgbc_objective(trial):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_uniform('learning_rate', 0.0001, 0.99),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000, step=100),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1),\n",
    "        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),     \n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 15),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0, log=True),\n",
    "    }\n",
    "    xgbc = XGBClassifier(**params,n_jobs = -1, random_state=1234,\n",
    "                         use_label_encoder=False, objective = \"binary:logistic\",\n",
    "                        eval_metric = \"error\")\n",
    "    scores = cross_val_score(xgbc, X_train, y_train, cv=kfold, scoring=\"precision\")\n",
    "    pre_mean = scores.mean()\n",
    "    return pre_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-18 03:29:18,272]\u001b[0m A new study created in memory with name: no-name-5247886e-85c9-47b3-be67-500a079e4708\u001b[0m\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_3880/3441796070.py:4: FutureWarning:\n",
      "\n",
      "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_3880/3441796070.py:9: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_3880/3441796070.py:10: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_3880/3441796070.py:11: FutureWarning:\n",
      "\n",
      "suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "\n",
      "c:\\Users\\SJM\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning:\n",
      "\n",
      "`use_label_encoder` is deprecated in 1.7.0.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 03:31:51,792]\u001b[0m Trial 0 finished with value: 0.7518267357366285 and parameters: {'max_depth': 8, 'learning_rate': 0.17383210686133077, 'n_estimators': 5600, 'colsample_bytree': 0.5998615993068929, 'colsample_bylevel': 0.9702933157822344, 'colsample_bynode': 0.8206897062488028, 'reg_lambda': 0.16269509823619246, 'reg_alpha': 0.024794069926550935, 'subsample': 0.9, 'min_child_weight': 4, 'gamma': 0.42514606493873014}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:34:16,087]\u001b[0m Trial 1 finished with value: 0.7127830126856374 and parameters: {'max_depth': 4, 'learning_rate': 0.5536398613444427, 'n_estimators': 7800, 'colsample_bytree': 0.7244117885493968, 'colsample_bylevel': 0.6572686652830027, 'colsample_bynode': 0.8149619686251361, 'reg_lambda': 0.09509675114433486, 'reg_alpha': 0.5540269195303126, 'subsample': 0.95, 'min_child_weight': 3, 'gamma': 0.34852433515755255}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:34:49,725]\u001b[0m Trial 2 finished with value: 0.6849176319485664 and parameters: {'max_depth': 8, 'learning_rate': 0.50080203371669, 'n_estimators': 1200, 'colsample_bytree': 0.7874293564991888, 'colsample_bylevel': 0.8167821162453424, 'colsample_bynode': 0.6452732060227945, 'reg_lambda': 0.085736126234789, 'reg_alpha': 0.010342404042009116, 'subsample': 0.75, 'min_child_weight': 2, 'gamma': 0.8303844003586013}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:37:29,368]\u001b[0m Trial 3 finished with value: 0.7248941276145644 and parameters: {'max_depth': 10, 'learning_rate': 0.3330861238384967, 'n_estimators': 9300, 'colsample_bytree': 0.5599225055733086, 'colsample_bylevel': 0.8302771843894796, 'colsample_bynode': 0.6162098249319874, 'reg_lambda': 0.039394723179614434, 'reg_alpha': 0.024073685486416953, 'subsample': 0.95, 'min_child_weight': 10, 'gamma': 0.2817059458808129}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:38:33,646]\u001b[0m Trial 4 finished with value: 0.6787971731051121 and parameters: {'max_depth': 4, 'learning_rate': 0.4041886368447554, 'n_estimators': 4100, 'colsample_bytree': 0.5368436761041169, 'colsample_bylevel': 0.589787631244936, 'colsample_bynode': 0.8773541562048855, 'reg_lambda': 0.07669037232468726, 'reg_alpha': 0.2064928293907774, 'subsample': 0.8, 'min_child_weight': 8, 'gamma': 0.1644080565325882}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:42:36,395]\u001b[0m Trial 5 finished with value: 0.7078473112552908 and parameters: {'max_depth': 6, 'learning_rate': 0.004314086203261748, 'n_estimators': 9400, 'colsample_bytree': 0.8962742150603461, 'colsample_bylevel': 0.6874694612804778, 'colsample_bynode': 0.8790343249628415, 'reg_lambda': 0.6239496947491475, 'reg_alpha': 0.3532038649743635, 'subsample': 0.6, 'min_child_weight': 14, 'gamma': 0.6600384203266246}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:43:55,943]\u001b[0m Trial 6 finished with value: 0.6417461308207291 and parameters: {'max_depth': 8, 'learning_rate': 0.49111445705036194, 'n_estimators': 5200, 'colsample_bytree': 0.6147469529521108, 'colsample_bylevel': 0.8911466134044772, 'colsample_bynode': 0.5383842916068483, 'reg_lambda': 0.428978351586459, 'reg_alpha': 0.015888648284781835, 'subsample': 0.6, 'min_child_weight': 6, 'gamma': 0.11954980446803368}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:45:44,322]\u001b[0m Trial 7 finished with value: 0.6421981503992901 and parameters: {'max_depth': 10, 'learning_rate': 0.3764747894340628, 'n_estimators': 5300, 'colsample_bytree': 0.8820544842649466, 'colsample_bylevel': 0.6495657813379181, 'colsample_bynode': 0.6872285479031828, 'reg_lambda': 0.1868774546229577, 'reg_alpha': 0.20644407314904373, 'subsample': 0.6, 'min_child_weight': 7, 'gamma': 0.5039645683367943}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:48:14,577]\u001b[0m Trial 8 finished with value: 0.6127329376508538 and parameters: {'max_depth': 6, 'learning_rate': 0.776168221628531, 'n_estimators': 9000, 'colsample_bytree': 0.9128816971871503, 'colsample_bylevel': 0.7947770324258374, 'colsample_bynode': 0.7954889488525513, 'reg_lambda': 0.045989215379612605, 'reg_alpha': 0.10788077163301243, 'subsample': 0.7, 'min_child_weight': 9, 'gamma': 0.15486266414349495}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:50:30,082]\u001b[0m Trial 9 finished with value: 0.7034552277683269 and parameters: {'max_depth': 8, 'learning_rate': 0.33996674372405633, 'n_estimators': 6800, 'colsample_bytree': 0.9495677885468008, 'colsample_bylevel': 0.8136676154091048, 'colsample_bynode': 0.782383405047155, 'reg_lambda': 0.023917702458030137, 'reg_alpha': 0.13720070446664068, 'subsample': 0.85, 'min_child_weight': 15, 'gamma': 0.2829534655630003}. Best is trial 0 with value: 0.7518267357366285.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:51:47,758]\u001b[0m Trial 10 finished with value: 0.7612063925715423 and parameters: {'max_depth': 6, 'learning_rate': 0.014799943698625084, 'n_estimators': 2800, 'colsample_bytree': 0.6562397867495172, 'colsample_bylevel': 0.992160974702642, 'colsample_bynode': 0.9966747989429885, 'reg_lambda': 0.01018493307992894, 'reg_alpha': 0.04252126615058837, 'subsample': 1.0, 'min_child_weight': 4, 'gamma': 0.44909794708848505}. Best is trial 10 with value: 0.7612063925715423.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:53:03,155]\u001b[0m Trial 11 finished with value: 0.7607503677432106 and parameters: {'max_depth': 6, 'learning_rate': 0.015758707168897157, 'n_estimators': 2800, 'colsample_bytree': 0.6618678016862781, 'colsample_bylevel': 0.9538636603770411, 'colsample_bynode': 0.9826626248551205, 'reg_lambda': 0.013384539798681096, 'reg_alpha': 0.04023461203933056, 'subsample': 1.0, 'min_child_weight': 4, 'gamma': 0.4395879688953096}. Best is trial 10 with value: 0.7612063925715423.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:54:07,308]\u001b[0m Trial 12 finished with value: 0.7636554119928853 and parameters: {'max_depth': 6, 'learning_rate': 0.04369084894670462, 'n_estimators': 2300, 'colsample_bytree': 0.7289624638159806, 'colsample_bylevel': 0.9910312229846223, 'colsample_bynode': 0.9987776549053358, 'reg_lambda': 0.013145114420001778, 'reg_alpha': 0.0446961795188397, 'subsample': 1.0, 'min_child_weight': 5, 'gamma': 0.5772925870526874}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:54:44,846]\u001b[0m Trial 13 finished with value: 0.7447157506622742 and parameters: {'max_depth': 5, 'learning_rate': 0.14993775709828522, 'n_estimators': 1500, 'colsample_bytree': 0.7792282941486739, 'colsample_bylevel': 0.997293743942922, 'colsample_bynode': 0.9901688501741801, 'reg_lambda': 0.011124537828704768, 'reg_alpha': 0.05146563851358943, 'subsample': 1.0, 'min_child_weight': 5, 'gamma': 0.9548837514373654}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:56:00,551]\u001b[0m Trial 14 finished with value: 0.7330662297396392 and parameters: {'max_depth': 7, 'learning_rate': 0.14756495516378637, 'n_estimators': 3000, 'colsample_bytree': 0.6974869938485728, 'colsample_bylevel': 0.8961799091805974, 'colsample_bynode': 0.92348864222597, 'reg_lambda': 0.021396258904242814, 'reg_alpha': 0.059592489643558955, 'subsample': 0.9, 'min_child_weight': 12, 'gamma': 0.6274459873763157}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:56:51,704]\u001b[0m Trial 15 finished with value: 0.6628508904513598 and parameters: {'max_depth': 5, 'learning_rate': 0.9764070409171153, 'n_estimators': 2700, 'colsample_bytree': 0.8374700392096202, 'colsample_bylevel': 0.52127712191331, 'colsample_bynode': 0.9323861266466996, 'reg_lambda': 0.010035135668804094, 'reg_alpha': 0.06384478324603561, 'subsample': 1.0, 'min_child_weight': 2, 'gamma': 0.22634486088528816}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:58:36,281]\u001b[0m Trial 16 finished with value: 0.7301819527396627 and parameters: {'max_depth': 7, 'learning_rate': 0.22727424630163734, 'n_estimators': 3800, 'colsample_bytree': 0.6564556616078254, 'colsample_bylevel': 0.9118330126205385, 'colsample_bynode': 0.9975489875313414, 'reg_lambda': 0.023253999378087937, 'reg_alpha': 0.02572968524167592, 'subsample': 0.9, 'min_child_weight': 6, 'gamma': 0.6414366507597492}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 03:59:22,826]\u001b[0m Trial 17 finished with value: 0.6927305217952743 and parameters: {'max_depth': 5, 'learning_rate': 0.6994933574088734, 'n_estimators': 1900, 'colsample_bytree': 0.9955652083679545, 'colsample_bylevel': 0.7512836899049087, 'colsample_bynode': 0.9058464374626808, 'reg_lambda': 0.040770740453285846, 'reg_alpha': 0.09208217475125319, 'subsample': 0.95, 'min_child_weight': 5, 'gamma': 0.5114972641846407}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:01:02,544]\u001b[0m Trial 18 finished with value: 0.73028797627708 and parameters: {'max_depth': 6, 'learning_rate': 0.08196115692501957, 'n_estimators': 4300, 'colsample_bytree': 0.818750694947251, 'colsample_bylevel': 0.9292730098444603, 'colsample_bynode': 0.7002308800872041, 'reg_lambda': 0.01729236946651363, 'reg_alpha': 0.03757062969170646, 'subsample': 0.85, 'min_child_weight': 11, 'gamma': 0.36851248343891985}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:01:50,415]\u001b[0m Trial 19 finished with value: 0.6895252204297677 and parameters: {'max_depth': 9, 'learning_rate': 0.2726421236341861, 'n_estimators': 2100, 'colsample_bytree': 0.7397441495556167, 'colsample_bylevel': 0.86366014848303, 'colsample_bynode': 0.9537056480236343, 'reg_lambda': 0.03528122296725561, 'reg_alpha': 0.01258262309145311, 'subsample': 0.7, 'min_child_weight': 7, 'gamma': 0.2154267266934078}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:03:42,197]\u001b[0m Trial 20 finished with value: 0.7565193462028011 and parameters: {'max_depth': 7, 'learning_rate': 0.09683638565435088, 'n_estimators': 3600, 'colsample_bytree': 0.6858991026004647, 'colsample_bylevel': 0.9988232455395253, 'colsample_bynode': 0.8749382560494527, 'reg_lambda': 0.2931321576980068, 'reg_alpha': 0.9167507640060135, 'subsample': 1.0, 'min_child_weight': 4, 'gamma': 0.7952667155587105}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:04:59,852]\u001b[0m Trial 21 finished with value: 0.7566101456785841 and parameters: {'max_depth': 6, 'learning_rate': 0.021867738330791073, 'n_estimators': 2900, 'colsample_bytree': 0.6506814899398052, 'colsample_bylevel': 0.9527879890017258, 'colsample_bynode': 0.9968473892119085, 'reg_lambda': 0.014162159641373987, 'reg_alpha': 0.037273061964216675, 'subsample': 1.0, 'min_child_weight': 4, 'gamma': 0.46642118533726784}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:05:57,279]\u001b[0m Trial 22 finished with value: 0.7509049897793892 and parameters: {'max_depth': 5, 'learning_rate': 0.020950012925789897, 'n_estimators': 2400, 'colsample_bytree': 0.5913103958608883, 'colsample_bylevel': 0.9503867143363024, 'colsample_bynode': 0.9555886466055725, 'reg_lambda': 0.015092269134114555, 'reg_alpha': 0.043609695843052174, 'subsample': 0.95, 'min_child_weight': 3, 'gamma': 0.40331930858678816}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:06:33,257]\u001b[0m Trial 23 finished with value: 0.7420995484552406 and parameters: {'max_depth': 7, 'learning_rate': 0.22863045896794063, 'n_estimators': 1300, 'colsample_bytree': 0.6512829447389813, 'colsample_bylevel': 0.9984054180506021, 'colsample_bynode': 0.9582276056306253, 'reg_lambda': 0.02977107590303047, 'reg_alpha': 0.07122163364858161, 'subsample': 1.0, 'min_child_weight': 6, 'gamma': 0.5447876316004706}. Best is trial 12 with value: 0.7636554119928853.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:08:12,835]\u001b[0m Trial 24 finished with value: 0.7690965619158485 and parameters: {'max_depth': 6, 'learning_rate': 0.08804769290228191, 'n_estimators': 4600, 'colsample_bytree': 0.5014094497718158, 'colsample_bylevel': 0.8574989514358492, 'colsample_bynode': 0.8423500282155455, 'reg_lambda': 0.06350163925427106, 'reg_alpha': 0.01863725484785192, 'subsample': 0.95, 'min_child_weight': 3, 'gamma': 0.3119513352979096}. Best is trial 24 with value: 0.7690965619158485.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:09:47,098]\u001b[0m Trial 25 finished with value: 0.7555928925142872 and parameters: {'max_depth': 5, 'learning_rate': 0.11124629667479902, 'n_estimators': 4600, 'colsample_bytree': 0.5074946559716688, 'colsample_bylevel': 0.8635264329067078, 'colsample_bynode': 0.8415731659012656, 'reg_lambda': 0.061548964642876416, 'reg_alpha': 0.018535838535218663, 'subsample': 0.85, 'min_child_weight': 2, 'gamma': 0.3028544067762314}. Best is trial 24 with value: 0.7690965619158485.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:12:01,595]\u001b[0m Trial 26 finished with value: 0.7526002999929604 and parameters: {'max_depth': 7, 'learning_rate': 0.20602738780590446, 'n_estimators': 6500, 'colsample_bytree': 0.5090235955454295, 'colsample_bylevel': 0.7580502427535258, 'colsample_bynode': 0.7480754278394416, 'reg_lambda': 0.15914244472872607, 'reg_alpha': 0.029760036635190206, 'subsample': 0.95, 'min_child_weight': 3, 'gamma': 0.23742309242167503}. Best is trial 24 with value: 0.7690965619158485.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:13:14,518]\u001b[0m Trial 27 finished with value: 0.745665723840377 and parameters: {'max_depth': 6, 'learning_rate': 0.08344684241115861, 'n_estimators': 3400, 'colsample_bytree': 0.5615677285565362, 'colsample_bylevel': 0.8626885179913961, 'colsample_bynode': 0.7444495396575579, 'reg_lambda': 0.055715789218971704, 'reg_alpha': 0.017687370096188395, 'subsample': 0.9, 'min_child_weight': 5, 'gamma': 0.3451568086851594}. Best is trial 24 with value: 0.7690965619158485.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:14:54,341]\u001b[0m Trial 28 finished with value: 0.731804435227638 and parameters: {'max_depth': 4, 'learning_rate': 0.28236962090141365, 'n_estimators': 4700, 'colsample_bytree': 0.7691941531765167, 'colsample_bylevel': 0.9226270098658791, 'colsample_bynode': 0.9052084406952235, 'reg_lambda': 0.9333218685639846, 'reg_alpha': 0.08897493250600172, 'subsample': 0.95, 'min_child_weight': 8, 'gamma': 0.5779654881012927}. Best is trial 24 with value: 0.7690965619158485.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:20,830]\u001b[0m Trial 29 finished with value: 0.7457941679196661 and parameters: {'max_depth': 7, 'learning_rate': 0.17676669748930873, 'n_estimators': 6300, 'colsample_bytree': 0.6029752419878309, 'colsample_bylevel': 0.7045971129133506, 'colsample_bynode': 0.8387289131453439, 'reg_lambda': 0.01825410147610075, 'reg_alpha': 0.024128810618818122, 'subsample': 0.9, 'min_child_weight': 3, 'gamma': 0.40836465767268054}. Best is trial 24 with value: 0.7690965619158485.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgbc_study = optuna.create_study(direction=\"maximize\")\n",
    "xgbc_study.optimize(xgbc_objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : [0.7690965619158485]\n",
      "params : {'max_depth': 6, 'learning_rate': 0.08804769290228191, 'n_estimators': 4600, 'colsample_bytree': 0.5014094497718158, 'colsample_bylevel': 0.8574989514358492, 'colsample_bynode': 0.8423500282155455, 'reg_lambda': 0.06350163925427106, 'reg_alpha': 0.01863725484785192, 'subsample': 0.95, 'min_child_weight': 3, 'gamma': 0.3119513352979096}\n"
     ]
    }
   ],
   "source": [
    "# 최적의 values 및 params\n",
    "print(\"value :\", xgbc_study.best_trial.values)\n",
    "print(\"params :\", xgbc_study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8251173708920188"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 찾은 최적의 값으로 모델을 재학습\n",
    "opt_xgbc = XGBClassifier(n_jobs=-1,\n",
    "                            random_state=1234,\n",
    "                            n_estimators = xgbc_study.best_params[\"n_estimators\"],\n",
    "                            learning_rate = xgbc_study.best_params[\"learning_rate\"],\n",
    "                            max_depth = xgbc_study.best_params[\"max_depth\"],\n",
    "                            subsample = xgbc_study.best_params[\"subsample\"],\n",
    "                            colsample_bytree = xgbc_study.best_params[\"colsample_bytree\"],\n",
    "                            colsample_bylevel = xgbc_study.best_params[\"colsample_bylevel\"],\n",
    "                            colsample_bynode = xgbc_study.best_params[\"colsample_bynode\"],\n",
    "                            reg_alpha = xgbc_study.best_params[\"reg_alpha\"],\n",
    "                            reg_lambda = xgbc_study.best_params[\"reg_lambda\"],\n",
    "                            min_child_weight = xgbc_study.best_params[\"min_child_weight\"],\n",
    "                            gamma = xgbc_study.best_params[\"gamma\"]\n",
    "                                )\n",
    "opt_xgbc.fit(X_train, y_train)\n",
    "precision_score(y_test, opt_xgbc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgbc_model_best.pkl']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(opt_xgbc, \"xgbc_model_best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky_NzzcJtxIF"
   },
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna Lightgbm\n",
    "def gbm_objective (trial):\n",
    "    params ={\n",
    "            \"n_estimators\" : trial.suggest_int(\"n_estimators\", 100, 300, 2),\n",
    "            \"learning_rate\" : trial.suggest_float(\"learning_rate\", 0.1, 1, step=0.1),\n",
    "            \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 10),\n",
    "            \"min_child_samples\" : trial.suggest_int(\"min_child_samples\", 1, 10),\n",
    "            \"subsample\" : trial.suggest_uniform('subsample', 0.1, 1.0), \n",
    "            \"reg_alpha\" : trial.suggest_float(\"reg_alpha\", 0.1, 1.0, step=0.1),\n",
    "            \"reg_lambda\" : trial.suggest_float(\"reg_lambda\", 0.1, 1.0, step=0.1)\n",
    "             }\n",
    "    gbm = LGBMClassifier(n_jobs=-1, **params,random_state=1234)\n",
    "    score = cross_socre(gbm)\n",
    "    gbm_score = score.mean()\n",
    "    return gbm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-18 04:17:43,976]\u001b[0m A new study created in memory with name: no-name-a7ef7cd4-ff0e-4ac2-a108-b92cf397a9ff\u001b[0m\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_3880/2386643099.py:8: FutureWarning:\n",
      "\n",
      "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "\n",
      "\u001b[32m[I 2022-11-18 04:17:45,638]\u001b[0m Trial 0 finished with value: 0.711595593892431 and parameters: {'n_estimators': 266, 'learning_rate': 0.7000000000000001, 'max_depth': 5, 'min_child_samples': 9, 'subsample': 0.83713659353794, 'reg_alpha': 0.9, 'reg_lambda': 0.9}. Best is trial 0 with value: 0.711595593892431.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:47,791]\u001b[0m Trial 1 finished with value: 0.7213896802545829 and parameters: {'n_estimators': 294, 'learning_rate': 0.6, 'max_depth': 6, 'min_child_samples': 3, 'subsample': 0.35304978018960753, 'reg_alpha': 0.4, 'reg_lambda': 0.1}. Best is trial 1 with value: 0.7213896802545829.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:50,209]\u001b[0m Trial 2 finished with value: 0.7484141269445758 and parameters: {'n_estimators': 294, 'learning_rate': 0.4, 'max_depth': 5, 'min_child_samples': 6, 'subsample': 0.8845223830568495, 'reg_alpha': 0.7000000000000001, 'reg_lambda': 0.9}. Best is trial 2 with value: 0.7484141269445758.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:52,481]\u001b[0m Trial 3 finished with value: 0.752765848653664 and parameters: {'n_estimators': 228, 'learning_rate': 0.30000000000000004, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.8852184840184114, 'reg_alpha': 0.1, 'reg_lambda': 0.8}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:54,371]\u001b[0m Trial 4 finished with value: 0.726916788151075 and parameters: {'n_estimators': 238, 'learning_rate': 0.6, 'max_depth': 6, 'min_child_samples': 7, 'subsample': 0.61414257846408, 'reg_alpha': 0.8, 'reg_lambda': 0.8}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:56,826]\u001b[0m Trial 5 finished with value: 0.7510993042289587 and parameters: {'n_estimators': 292, 'learning_rate': 0.7000000000000001, 'max_depth': 5, 'min_child_samples': 6, 'subsample': 0.45523388669052234, 'reg_alpha': 0.1, 'reg_lambda': 0.7000000000000001}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:58,400]\u001b[0m Trial 6 finished with value: 0.7111528221247301 and parameters: {'n_estimators': 288, 'learning_rate': 0.8, 'max_depth': 4, 'min_child_samples': 1, 'subsample': 0.4109145523831965, 'reg_alpha': 0.6, 'reg_lambda': 0.5}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:17:59,385]\u001b[0m Trial 7 finished with value: 0.6925206194713611 and parameters: {'n_estimators': 214, 'learning_rate': 0.7000000000000001, 'max_depth': 3, 'min_child_samples': 4, 'subsample': 0.9791313466733056, 'reg_alpha': 0.1, 'reg_lambda': 0.8}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:00,866]\u001b[0m Trial 8 finished with value: 0.6841194006939465 and parameters: {'n_estimators': 172, 'learning_rate': 0.9, 'max_depth': 8, 'min_child_samples': 2, 'subsample': 0.5240339358439827, 'reg_alpha': 0.8, 'reg_lambda': 0.2}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:03,274]\u001b[0m Trial 9 finished with value: 0.7295920460541108 and parameters: {'n_estimators': 280, 'learning_rate': 0.4, 'max_depth': 7, 'min_child_samples': 5, 'subsample': 0.8379245716809423, 'reg_alpha': 0.9, 'reg_lambda': 0.8}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:03,684]\u001b[0m Trial 10 finished with value: 0.6318070818070819 and parameters: {'n_estimators': 106, 'learning_rate': 0.1, 'max_depth': 1, 'min_child_samples': 10, 'subsample': 0.1423143786600627, 'reg_alpha': 0.4, 'reg_lambda': 0.4}. Best is trial 3 with value: 0.752765848653664.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:06,398]\u001b[0m Trial 11 finished with value: 0.7781452012274738 and parameters: {'n_estimators': 176, 'learning_rate': 0.2, 'max_depth': 10, 'min_child_samples': 8, 'subsample': 0.6746473890548477, 'reg_alpha': 0.1, 'reg_lambda': 0.6}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:09,044]\u001b[0m Trial 12 finished with value: 0.7482507020097786 and parameters: {'n_estimators': 166, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_samples': 8, 'subsample': 0.6759278757924968, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 0.6}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:11,807]\u001b[0m Trial 13 finished with value: 0.7612219065508311 and parameters: {'n_estimators': 178, 'learning_rate': 0.30000000000000004, 'max_depth': 10, 'min_child_samples': 10, 'subsample': 0.7212804568514868, 'reg_alpha': 0.2, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:14,581]\u001b[0m Trial 14 finished with value: 0.7635137949321658 and parameters: {'n_estimators': 174, 'learning_rate': 0.30000000000000004, 'max_depth': 10, 'min_child_samples': 8, 'subsample': 0.7284864992425074, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:16,657]\u001b[0m Trial 15 finished with value: 0.7593629641181125 and parameters: {'n_estimators': 132, 'learning_rate': 0.2, 'max_depth': 9, 'min_child_samples': 8, 'subsample': 0.7273691998682842, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 0.30000000000000004}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:18,896]\u001b[0m Trial 16 finished with value: 0.7608571657152833 and parameters: {'n_estimators': 146, 'learning_rate': 0.4, 'max_depth': 8, 'min_child_samples': 8, 'subsample': 0.2577627648279035, 'reg_alpha': 0.5, 'reg_lambda': 0.6}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:21,952]\u001b[0m Trial 17 finished with value: 0.7585333562899497 and parameters: {'n_estimators': 198, 'learning_rate': 0.2, 'max_depth': 9, 'min_child_samples': 7, 'subsample': 0.5692406409362758, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:24,254]\u001b[0m Trial 18 finished with value: 0.7443745139006449 and parameters: {'n_estimators': 140, 'learning_rate': 0.5, 'max_depth': 10, 'min_child_samples': 7, 'subsample': 0.774859928711989, 'reg_alpha': 0.2, 'reg_lambda': 0.4}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:26,973]\u001b[0m Trial 19 finished with value: 0.755513394905767 and parameters: {'n_estimators': 188, 'learning_rate': 0.2, 'max_depth': 8, 'min_child_samples': 9, 'subsample': 0.9876356329481479, 'reg_alpha': 0.5, 'reg_lambda': 0.5}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:28,715]\u001b[0m Trial 20 finished with value: 0.6977723434268469 and parameters: {'n_estimators': 102, 'learning_rate': 1.0, 'max_depth': 9, 'min_child_samples': 5, 'subsample': 0.6707560781985492, 'reg_alpha': 0.2, 'reg_lambda': 0.7000000000000001}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:31,382]\u001b[0m Trial 21 finished with value: 0.7551243685530237 and parameters: {'n_estimators': 170, 'learning_rate': 0.30000000000000004, 'max_depth': 10, 'min_child_samples': 9, 'subsample': 0.7450242044321025, 'reg_alpha': 0.2, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:33,857]\u001b[0m Trial 22 finished with value: 0.7615611935208643 and parameters: {'n_estimators': 156, 'learning_rate': 0.30000000000000004, 'max_depth': 10, 'min_child_samples': 10, 'subsample': 0.6446686559679595, 'reg_alpha': 0.2, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:36,284]\u001b[0m Trial 23 finished with value: 0.7495731170392141 and parameters: {'n_estimators': 152, 'learning_rate': 0.5, 'max_depth': 9, 'min_child_samples': 9, 'subsample': 0.6145261064590489, 'reg_alpha': 0.4, 'reg_lambda': 0.9}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:38,055]\u001b[0m Trial 24 finished with value: 0.7193089307701059 and parameters: {'n_estimators': 124, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_samples': 8, 'subsample': 0.48943303650434283, 'reg_alpha': 0.1, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:41,276]\u001b[0m Trial 25 finished with value: 0.7610561743162211 and parameters: {'n_estimators': 210, 'learning_rate': 0.30000000000000004, 'max_depth': 10, 'min_child_samples': 10, 'subsample': 0.5941998170913402, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 0.7000000000000001}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:43,456]\u001b[0m Trial 26 finished with value: 0.7587598130059121 and parameters: {'n_estimators': 156, 'learning_rate': 0.2, 'max_depth': 7, 'min_child_samples': 7, 'subsample': 0.6549197601410074, 'reg_alpha': 0.2, 'reg_lambda': 0.9}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:46,370]\u001b[0m Trial 27 finished with value: 0.755098683468624 and parameters: {'n_estimators': 188, 'learning_rate': 0.4, 'max_depth': 9, 'min_child_samples': 9, 'subsample': 0.8061969626547512, 'reg_alpha': 0.1, 'reg_lambda': 0.4}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:46,775]\u001b[0m Trial 28 finished with value: 0.6040724189986978 and parameters: {'n_estimators': 118, 'learning_rate': 0.2, 'max_depth': 1, 'min_child_samples': 6, 'subsample': 0.5517815472106155, 'reg_alpha': 0.6, 'reg_lambda': 0.1}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:49,340]\u001b[0m Trial 29 finished with value: 0.7554105145594944 and parameters: {'n_estimators': 250, 'learning_rate': 0.5, 'max_depth': 8, 'min_child_samples': 8, 'subsample': 0.8789060215287479, 'reg_alpha': 0.4, 'reg_lambda': 0.9}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:49,940]\u001b[0m Trial 30 finished with value: 0.6201198069944731 and parameters: {'n_estimators': 158, 'learning_rate': 0.30000000000000004, 'max_depth': 2, 'min_child_samples': 10, 'subsample': 0.6923651725830172, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:52,860]\u001b[0m Trial 31 finished with value: 0.7629593190153983 and parameters: {'n_estimators': 186, 'learning_rate': 0.30000000000000004, 'max_depth': 10, 'min_child_samples': 10, 'subsample': 0.7390877066244339, 'reg_alpha': 0.2, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:55,830]\u001b[0m Trial 32 finished with value: 0.7514759150990798 and parameters: {'n_estimators': 184, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_samples': 9, 'subsample': 0.7888705924952903, 'reg_alpha': 0.2, 'reg_lambda': 1.0}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:18:58,979]\u001b[0m Trial 33 finished with value: 0.750897887274618 and parameters: {'n_estimators': 200, 'learning_rate': 0.4, 'max_depth': 9, 'min_child_samples': 10, 'subsample': 0.6411317445998895, 'reg_alpha': 0.1, 'reg_lambda': 0.9}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:02,304]\u001b[0m Trial 34 finished with value: 0.7723917190489502 and parameters: {'n_estimators': 218, 'learning_rate': 0.30000000000000004, 'max_depth': 10, 'min_child_samples': 9, 'subsample': 0.8549428164351764, 'reg_alpha': 0.2, 'reg_lambda': 0.9}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:05,579]\u001b[0m Trial 35 finished with value: 0.765845266659194 and parameters: {'n_estimators': 222, 'learning_rate': 0.2, 'max_depth': 9, 'min_child_samples': 8, 'subsample': 0.9241693369229124, 'reg_alpha': 1.0, 'reg_lambda': 0.9}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:08,773]\u001b[0m Trial 36 finished with value: 0.7488675805953756 and parameters: {'n_estimators': 228, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_samples': 7, 'subsample': 0.9350732311619475, 'reg_alpha': 0.9, 'reg_lambda': 0.8}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:12,709]\u001b[0m Trial 37 finished with value: 0.7668806129908103 and parameters: {'n_estimators': 252, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.9240579214355049, 'reg_alpha': 1.0, 'reg_lambda': 0.8}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:16,508]\u001b[0m Trial 38 finished with value: 0.7609230716911796 and parameters: {'n_estimators': 252, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 4, 'subsample': 0.9329891228362963, 'reg_alpha': 1.0, 'reg_lambda': 0.7000000000000001}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:20,673]\u001b[0m Trial 39 finished with value: 0.7644009026853553 and parameters: {'n_estimators': 266, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.8447286027493927, 'reg_alpha': 1.0, 'reg_lambda': 0.8}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:22,574]\u001b[0m Trial 40 finished with value: 0.716699253437311 and parameters: {'n_estimators': 220, 'learning_rate': 0.6, 'max_depth': 7, 'min_child_samples': 5, 'subsample': 0.9258100344047898, 'reg_alpha': 0.7000000000000001, 'reg_lambda': 0.6}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:26,469]\u001b[0m Trial 41 finished with value: 0.7713240632714493 and parameters: {'n_estimators': 258, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.8846994006896775, 'reg_alpha': 1.0, 'reg_lambda': 0.8}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:30,088]\u001b[0m Trial 42 finished with value: 0.7563687579191003 and parameters: {'n_estimators': 240, 'learning_rate': 0.2, 'max_depth': 8, 'min_child_samples': 4, 'subsample': 0.8892634606451341, 'reg_alpha': 1.0, 'reg_lambda': 0.9}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:34,313]\u001b[0m Trial 43 finished with value: 0.7690109161549853 and parameters: {'n_estimators': 268, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 7, 'subsample': 0.8697761220813574, 'reg_alpha': 0.8, 'reg_lambda': 0.8}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:38,409]\u001b[0m Trial 44 finished with value: 0.7578564045790206 and parameters: {'n_estimators': 272, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.8403991169460653, 'reg_alpha': 0.8, 'reg_lambda': 0.7000000000000001}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:42,697]\u001b[0m Trial 45 finished with value: 0.763696896869593 and parameters: {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 7, 'subsample': 0.9718731906044299, 'reg_alpha': 0.8, 'reg_lambda': 0.8}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:43,843]\u001b[0m Trial 46 finished with value: 0.6629641276997903 and parameters: {'n_estimators': 254, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 6, 'subsample': 0.8820606633631113, 'reg_alpha': 0.9, 'reg_lambda': 0.6}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:45,701]\u001b[0m Trial 47 finished with value: 0.7413135175947805 and parameters: {'n_estimators': 274, 'learning_rate': 0.2, 'max_depth': 4, 'min_child_samples': 5, 'subsample': 0.3518729398807172, 'reg_alpha': 0.7000000000000001, 'reg_lambda': 0.8}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:47,401]\u001b[0m Trial 48 finished with value: 0.6942932536522084 and parameters: {'n_estimators': 260, 'learning_rate': 0.8, 'max_depth': 7, 'min_child_samples': 3, 'subsample': 0.8140173256995777, 'reg_alpha': 0.9, 'reg_lambda': 0.7000000000000001}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n",
      "\u001b[32m[I 2022-11-18 04:19:50,504]\u001b[0m Trial 49 finished with value: 0.7689424775034147 and parameters: {'n_estimators': 240, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_samples': 7, 'subsample': 0.8589025413989154, 'reg_alpha': 0.9, 'reg_lambda': 0.5}. Best is trial 11 with value: 0.7781452012274738.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# optuna study\n",
    "gbm_study = optuna.create_study(direction = \"maximize\")\n",
    "gbm_study.optimize(gbm_objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : [0.7781452012274738]\n",
      "params : {'n_estimators': 176, 'learning_rate': 0.2, 'max_depth': 10, 'min_child_samples': 8, 'subsample': 0.6746473890548477, 'reg_alpha': 0.1, 'reg_lambda': 0.6}\n"
     ]
    }
   ],
   "source": [
    "# 최적의 values 및 params\n",
    "print(\"value :\", gbm_study.best_trial.values)\n",
    "print(\"params :\", gbm_study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor : 0.5869175627240143\n"
     ]
    }
   ],
   "source": [
    "# optuna\n",
    "gbm_study_best = LGBMClassifier(n_jobs=-1,\n",
    "                                n_estimators = gbm_study.best_trial.params[\"n_estimators\" ],\n",
    "                                learning_rate = gbm_study.best_trial.params[\"learning_rate\"],\n",
    "                                max_depth = gbm_study.best_trial.params[\"max_depth\"],\n",
    "                                min_child_samples = gbm_study.best_trial.params[\"min_child_samples\"],\n",
    "                                subsample = gbm_study.best_trial.params[\"subsample\"],\n",
    "                                reg_alpha = gbm_study.best_trial.params[\"reg_alpha\"],\n",
    "                                reg_lambda = gbm_study.best_trial.params[\"reg_lambda\"],\n",
    "                                random_state = 1234,\n",
    "                                      )\n",
    "gbm_study_best.fit(X_train, y_train)\n",
    "gbm_study_ypred = gbm_study_best.predict(X_test)\n",
    "print(\"LGBMRegressor :\", precision_score(gbm_study_ypred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gbm_model_best.pkl']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장 \n",
    "joblib.dump(gbm_study_best, \"gbm_model_best.pkl\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "443df816ca8b48c8733e4bc28c967fd9649d4d5e864a98e2da4f1120f5b63237"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
